%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[nocopyrightspace]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,color,listings,lstcoq,url}
\usepackage[T1]{fontenc}

\definecolor{ltblue}{rgb}{0,0.4,0.4}
\definecolor{dkblue}{rgb}{0,0.1,0.6}
\definecolor{dkgreen}{rgb}{0,0.35,0}
\definecolor{dkviolet}{rgb}{0.3,0,0.5}
\definecolor{dkred}{rgb}{0.5,0,0}

\begin{document}

\lstset{language=coq, basicstyle=\ttfamily\scriptsize, columns=flexible,
keepspaces=true}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Formalizing the Honeynet}
\subtitle{Defining and Proving a Rootkit Installation}

\authorinfo{CM Lubinski}
           {DePaul University}
           {cm.lubinski@gmail.com}

\maketitle

\begin{abstract}
Formal definitions of multiple file formats, ``rootkit-like'' tar files, and
event timelines are provided in the proof-oriented language, Coq. These are
combined to mimic the types of evidence given by independent forensics
researchers in a forensics competition (``Honeynet''.) Using said definitions,
the evidence is proven to be consistent with the disk image provided for the
relevant competition. Along the way, functions for parsing and manipulating
pertinent data structures within Coq are also described. The wider feasibility
of this approach is also discussed.
\end{abstract}

\keywords
forensics, formalization, Coq, Honeynet, Ext2

\section{Introduction*}

After completing a lengthy customs process in the largest airport of
burgeoning global power, you notice that your laptop appears to be running
rather slowly. You suspect that malicious software has been installed by the
country's authorities, but are uncertain how to prove this. At first, you
consider using off-the-shelf antivirus software to find the infection, but
realize that the risk of damaging international relations is far too great to
rely on the definitions within the proprietary software. What you really need
is a concrete, standardized definition of what it means to have malware
installed and evidence fitting that definition to prove that your machine has
been invaded.

Solutions to this scenario and others like it (though, perhaps more mundane)
are the ultimate goal of the research provided in this paper. To achieve that,
we need formal definitions for various types of evidence used by forensics
analysts.  We also require a way to convert real data (e.g. from a disk image)
into these definitions such that we can provide concrete {\em proofs} showing
that a particular image does or does not fit such a definition. In the above
example, we could provide a verifiable proof that the disk image satisfied a
definition of ``malware installation''. 

To guide our search, we will focus on the evidence structures and proofs
described by several independent researchers studying a ``Honeynet''
challenge. The Honeynet Project\cite{honeynet}'s now-defunct {\it Scan of the
Month} series provided researchers a disk image attained from a compromised
honeypot (a computer created with the explicit goal of catching malware for
inspection.) Each month, participants were challenged to describe what
happened to the system and provide evidence for their conclusions. We will
consider one specific contest\cite{honeynet-15}, in which a rootkit
(replacement system programs that hide malicious activity) was installed on a
server. The security community was asked to recover the rootkit, prove that it
had been installed, and provide a step-by-step writeup describing how the
rootkit was found. We will formalize some of the definitions and evidence
provided by contestants in that competition.

Our definitions, parsers, and evidence will be described/implemented in the
proof-oriented language, Coq. While this language lives in the ML-family of
programming languages, we will augment it with several functional and
object-oriented notations. Using Coq allows us to formalize our proofs and
definitions, but significantly restricts the practicality of our efforts; we
will expand on these limitations throughout the paper. Despite these faults,
Coq lets us build complete proof terms which serve as evidence that the disk
image provided satisfies the researchers' definitions.

Finally, we will show how the proof terms generated allow our solution to be
{\em feasible} in the sense that they grow with the complexity of their
sub-goals rather than with the size of a disk image, file, etc. Combining the
specific evidence with documentation for the feasibility of this strategy
should demonstrate the approach's potential as a core tool for forensics
investigators. Along the way, we will include definitions of tar files,
rootkit-like archives, file deletion (on the Ext2 file system,) as well as a
timeline of events consistent with rootkit installation.

\section{Getting Our Feet Wet with File Types}

Let us start by considering a relatively straight-forward request: defining
what it means for a given file to be a JPEG. How can we formalize this notion?
One tact (used by many operating systems) is to rely on the file extension --
in this case, checking for either {\tt jpg} or {\tt jpeg}. This is a very
loose definition, however, as malicious users need only give their files a
different extension to avoid detection. We could instead review the JPEG spec
and confirm that all of the meta data contained within this file is consistent
with said spec. That approach runs the opposite end of the spectrum, requiring
significantly more evidence. Further, the JPEG spec is not as tidy as one
might hope; most applications are lenient with the file formats they accept
and messy with the files they write.

Instead, we will choose a middle route, opting to use ``magic numbers'' as our
guide. This term refers to tell-tale byte values at predictable offsets within
the file data associated with certain file types. These file signatures are
relatively unique to various file formats, so we will use them in our
definitions and add additional checks as necessary. JPEGs happen to always
start with the bytes {\tt ff d8} and end with {\tt ff d9}. Similarly, gzipped
files begin with {\tt 1f 8b 08} and Linux executables (ELFs) begin with {\tt
7f 45 4c 46} ({\tt 7f} E L F.)

Writing these definitions in Coq can be relatively straight forward. We need
only compare the bytes found within a file (given the context of a specific
disk image) to these magic numbers. We will use Coq's {\tt ascii} type to
represent a byte; its literal syntax is either a single ASCII character of a
three-digit unsigned value. With some Python-like syntactic sugar, this can
look like

\begin{lstlisting}
Definition isJpeg (file: File) (disk: Disk) :=
     file @[  0 | disk ] = Found "255"
  /\ file @[  1 | disk ] = Found "216"
  /\ file @[- 2 | disk ] = Found "255"
  /\ file @[- 1 | disk ] = Found "217".

Definition isGzip (file: File) (disk: Disk) :=
     file @[ 0 | disk ] = Found "031"
  /\ file @[ 1 | disk ] = Found "139" 
  /\ file @[ 2 | disk ] = Found "008".

Definition isElf (file: File) (disk: Disk) :=
     file @[ 0 | disk ] = Found "127"
  /\ file @[ 1 | disk ] = Found "E"
  /\ file @[ 2 | disk ] = Found "L"
  /\ file @[ 3 | disk ] = Found "F".
\end{lstlisting}

For each definition, the first (and, in the case of the JPEG, last) bytes of a
file must both be present (as indicated by {\tt Found}) and equal to the
unsigned byte values shown. We will discuss the absence of byte values later,
but for now, assume that this accounts for ``missing'' evidence which might
arise if a disk were damaged or inconsistent. By defining file types in this
manner, we can use these definitions as building blocks within larger terms
and within proofs. For example, we can now {\it prove} that JPEG files cannot
also be gzipped files:

\begin{lstlisting}
Lemma jpeg_is_not_gzip : forall (file: File) (disk: Disk),
  (isJpeg file disk) -> ~(isGzip file disk).
\end{lstlisting}

The tactics used to satisfy this proof (and others mentioned throughout this
paper) can be found in the code referenced in the appendix.

\section{A Definition of Rootkit Installation}

Let's now consider the Honeynet {\it Scan of the Month} mentioned in the
introduction. In his entry for this contest, Matt 
Borland\cite{borland-honeynet} described a deleted, ``tar/gzipped file
containing the tools necessary for creating a home for the attacker on the
compromised system''. We formalize this definition a bit and will say that a
definition of rootkit installations looks like

\begin{lstlisting}
Definition borland_rootkit (disk: Disk) (gunzip: File->File) :=
  exists file: File,
    isOnDisk file disk
    /\ isDeleted file
    /\ isGzip file disk
    /\ Tar.looksLikeRootkit (gunzip file) disk.
\end{lstlisting}

That's a bit of a mouthful; we are stating that there exists a deleted,
gzipped file on the Honeynet disk image such that, when we unzip that file,
the contained tar looks like a rootkit. If such a file exists, the Borland
definition is satisfied, and we presume a rootkit has been installed. For the
remainder of this section, we dive deeper into each of the definitions in the
``and'' clause (save {\tt isGzip}, which we have previously described.)
Section \ref{sec:sufficient} will investigate whether or not Borland's
definition is sufficient to describe a rootkit installation.

\subsection{The File Structure}

File systems, such as Ext2, FAT32, and NTFS, each represent ``files''
differently, yet there are several universal (and somewhat-universal)
properties shared across these systems. We use these shared properties to
describe a generic, abstract file structure which can be used in proofs
without worrying about the underlying file system. Many properties of files
(such as file type, as described above) should not depend on file system
traits.

What might these ``universal'' attributes of a file be? File size (in bytes)
seems appropriate, as does deletion status (on most file systems, ``deleted''
files have a tendency to remain present on disk.) We also need a file
identifier (which will be file system dependent) to distinguish this file from
others on disk. Finally, we have four common-but-not-universal fields -
access, modification, creation, and deletion times. These timestamps are
represented as natural numbers ({\tt N}), and are all optional (indicated by
{\tt Exc},) as not all file systems record their values. The optional fields
are particularly useful for constructing event time lines, as described in
Section \ref{sec:timelines}.

\begin{lstlisting}
Structure File := mkFile {
  fileId: FileId;
  fileSize: N;
  deleted: bool;
  lastAccess: Exc N;
  lastModification: Exc N;
  lastCreated: Exc N;
  lastDeleted: Exc N
}.
\end{lstlisting}

The {\tt FileId} type may take multiple forms; for the purposes of our
exploration, we care only about Ext2 file systems, files embedded in a
tar-archive, and files which we define from thin-air (see ``assumptions'' as
defined in Section \ref{sec:proving}.) The former needs an Inode Index to
serve as an identifier; the second needs the archive's file identifier as well
as an offset within the archive; the final will use {\tt ByteData} (i.e.  the
file data) to identify.

\begin{lstlisting}
Inductive FileId: Type :=
  | Ext2Id: N -> FileId
  | TarId: FileId -> N -> FileId
  | MockId: ByteData -> FileId
.
\end{lstlisting}

What about the file contents? The syntax we used before for accessing file
data ({\tt file @[ offset | disk ]}) gets expanded into a file-system-aware
function, {\tt fetchByte}. This function uses the provided disk and file
identifier to retrieve a requested byte, decoupling the file data from the
{\tt File} structure. For Ext2, this means delegating to the appropriate Ext2
function (described in Section \ref{sec:ext2}); for the Mock file system, it
means delegating to the contained {\tt ByteData}; Tar file processing will be
explained in Section \ref{sec:tar}.

\begin{lstlisting}
Fixpoint fetchByte (fileId: FileId) (disk: Disk)
  : N->@Fetch Byte := 
  match fileId with
  | Ext2Id inodeIndex => Ext2.fileByte disk inodeIndex
  | MockId data => data
  | TarId fs shiftAmt => fetchByte fs (shift disk shiftAmt)
  end.
\end{lstlisting}

In earlier work, {\tt File}s carried around a closure containing the relevant
file system data -- while this was more elegant, printing and constructing
{\tt File}s proved to be too unmanageable. We have instead settled on a simple
structure which is not directly tied to a disk image. To make the association
between images and files, we must require the {\tt isOnDisk} property.

\subsection{isOnDisk}

How might we define that a file "exists" on a particular disk image? For a
simple definition, we could start by claiming a ``file'' is on a disk image if
that file's contents could be found sequentially on the disk. In other words,
we might consider the file to be on disk if we can find a starting offset such
that every byte afterwards matches that of the file.

\begin{lstlisting}
Definition isOnDiskTry1 (file: File) (disk: Disk):
  exists (start: N), forall (i: N),
  i < file.(fileSize) -> 
    file @[ i | disk ] = disk (start + i).
\end{lstlisting}

This definition isn't very useful in practice, however, as files are very
often fragmented across multiple, disjoint segments of a disk. Moreover, this
definition has high potential for false positives, where a ``file'' is formed
by looking at the bits that span a fragment boundary but are not part of the
same (logical) byte sequence. Files are stored on disks via file systems,
which make no sequential guarantees (particularly in recent file systems such
as ZFS and Btrfs.) On spinning hard drives, the need to ``defragment'' arises
from the segmentation of files over non-adjacent sectors on disk; moving the
fragments to be adjacent improves read performance by reducing the times the
drive head must skip around.

As sequential access will not do, we must build our definition for file
existence with file systems in mind. Proving that a file exists within each
type of file system is a relatively unique operation, however, so we will
perform case analysis. While this research focuses on Ext2, we provide the
skeleton for other file systems below.

\begin{lstlisting}
Definition isOnDisk (file: File) (disk: Disk):
  match file.(fileId) with
  | Ext2Id inodeIndex =>
    (Ext2.findAndParseFile disk inodeIndex) = Found file
  | Fat32Id clusterNumber =>
    (Fat32.findAndParseFile disk clusterNumber)
      = Found file
  | BtrfsId key =>
    (Btrfs.findAndParseFile disk key) = Found file
  | ...
  | _ => False
  end.
\end{lstlisting}

The functions {\tt findAndParseFile} perform the work of inspecting the disk
image, reading the relevant data structures, and creating an abstract
representation of the results (in this case, a {\tt File}.) {\tt Found}
implies that there were no errors while trying to retrieve the file; see
Section \ref{subsec:Fetch}'s discussion of the {\tt Fetch} construction for
more.

\subsection{isDeleted}

As our file structure includes a {\tt deleted} boolean value, it should be
easy to see that the definition of {\tt isDeleted} need only delegate to that
field:

\begin{lstlisting}
Definition isDeleted (file: File) :=
  file.(deleted) = true.
\end{lstlisting}

\subsection{Gunzip, an External Function}

Before we can describe {\tt Tar.looksLikeRootkit}, we should note its first
parameter {\tt (gunzip file)}. {\tt gunzip}, a parameter to the {\tt
borland\_rootkit} definition, represents the unzipping/deflating operation for
gzip-files. As implementing the decompression algorithm within Coq would not
be particularly useful to our study, we instead relegate its operation to a
parameter in the definition. As we will see in Section \ref{subsec:gunzip},
this parameter percolates to the top of the proof; we will effectively run the
function via an external program in a preprocessing step.

\subsection{Tars That Look Like Rootkits}

The final clause in our definition requires that the provided, unzipped file
``looks like a rootkit''. Borland described the contents of the archive as
containing ``necessary'' files for a rootkit installation; he went on to
provide a list of the file names contained in the archive as evidence. We will
intuit that this list was relevant because the file names within the archive
looked to match those of system files. To reduce our false-positive rate, we
will require that at least two of the file names contained within the tar
appear to be system files.

\begin{lstlisting}
Definition looksLikeRootkit (file: File) (disk: Disk) :=
  let fileNames := Tar.parseFileNames file disk in
  exists (filename1 filename2: string),
       filename1 <> filename2
    /\ In filename1 fileNames
    /\ In filename2 fileNames
    /\ (FileNames.systemFile filename1)
    /\ (FileNames.systemFile filename2).
\end{lstlisting}

System file names are distinguished from other files in that they (excluding
their path prefixes) are in a predefined list. This list includes task
managers, such as {\tt top} and {\tt ProcMon.exe}, which rootkits replace to
hide their activities, as well as {\tt ssh} and {\tt rsync}, which grants them
the ability to monitor network traffic.

\begin{lstlisting}
Definition systemFile (fileName: string) :=
  In (trimFileNamePrefix fileName)
     ("ps" :: "netstat" :: "top" :: "ifconfig" :: "ssh"
           :: "rsync" :: "ProcMon.exe" :: nil).
\end{lstlisting}

Use of this definition is straight forward:

\begin{lstlisting}
Lemma systemFile_ex1 : 
  systemFile "last/top".

Lemma systemFile_ex2 : 
  ~(systemFile "last/example.txt").
\end{lstlisting}

\subsection{Summary*}

We have now described {\tt isOnDisk}, {\tt isDeleted}, {\tt isGzip}, and {\tt
Tar.looksLikeRootkit} enough that we could see why Borland proposed them as
definitions of evidence for a rootkit installation. Along the way, we glossed
over several critical parsing/computation functions (including retrieval of
bytes in the Ext2 file system and parsing file names from Tar files,) which we
will cover in sections \ref{sec:ext2} and \ref{sec:tar}. Before diving in,
however, we should clarify the relationships between some of the concepts we
are proposing; this is the focus of the next section.

\section{Proving the Property for a Particular Disk+}
\label{sec:proving}

Now that we have a definition for rootkit installation, we can prove that the
disk image provided for the Honeynet competition satisfies that definition. We
will use Coq's tactic language to build a proof term to serve as evidence.

\begin{lstlisting}
Lemma borland_honeynet_file:
  borland_rootkit honeynet_image_a gunzip_a.
  Proof.
    apply borland_reflection 
      with (file := file23) 
           (filename1 := maliciousFileName1) 
           (filename2 := maliciousFileName2).
    vm_compute. reflexivity.
Qed.
\end{lstlisting}

This very simply proof clearly hides a great deal of complexity. What are {\tt
honeynet\_image\_a} and {\tt gunzip\_a}? How does {\tt borland\_reflection}
work? Where did {\tt file23} and the \\{\tt maliciousFileName}s come from?
While we will answer each of these questions in detail, it's important to
recognize that writing the individual proof is very straight forward (and
could easily be automated). This plays a heavy role in the future of this type
of research, as discussed in section \ref{sec:future}. Further, the proof term
itself is quite small, the importance of which will be covered in section
\ref{subsec:reflection}.

\subsection{Disk Images}

{\tt honeynet\_image\_a} has the type, {\tt Disk}, which is used to represent
a random-access disk throughout our definitions and proofs.  Technically, it
is function from {\tt N} to {\tt Fetch Byte}, acting as a byte-retrieval
mechanism; give a {\tt Disk} an offset, and it will respond with the byte
value at the offset requested or an error code. The {\tt ByteData} type is
simply an alias of {\tt Disk}, but provides a more semantic name.

Generally, disk data is backed by an in-memory mapping via Coq's {\tt FMapAVL}
trees, and must be generated from the true disk values. Due to Coq's
limitations on data structure sizes, we only populate these maps with
essential values; once the map reaches a few thousand entries, it will no
longer be usable within Coq. We leave a full accounting of the disk bytes to
our code (see the comments in {\tt example\_images.v} for an explanation of
why each byte range is included.) Note: {\tt find} is an operation to reach
into a {\tt FMapAVL}.

\begin{lstlisting}
Definition honeynet_map := 
[ 
  1024 |-> ("216":Byte), 
  1025 |-> ("002":Byte),
  1026 |-> ("001":Byte), 
  1027 |-> ("000":Byte), 
  ...
].

Definition Disk_of_Map_N_Byte (map: Map_N_Byte) : Disk :=
  fun (key: Z) => find key map.

Definition honeynet_image_a : Disk := 
  Disk_of_Map_N_Byte honeynet_map.
\end{lstlisting}

\subsection{The Fetch Type and Populating Disk Images+}
\label{subsec:fetch}

As disk images are too large to place in memory, the {\tt Disk} discussed
above will have "holes." We initially gravitated towards the Option monad
({\tt Exc} in Coq,) but realized that, if we were to chain options (see
\ref{subsec:ext2functional},) we would have no feedback on {\em where} the
retrieval operations failed. We therefore created a new inductive type {\tt
Fetch}, which can be instantiated as a {\tt Found} when the byte (or whatever
data type) is present, a {\tt MissingAt} when not present, and an {\tt
ErrorString} when a different error occurs.

\begin{lstlisting}
Inductive Fetch {A:Type}: Type :=
  | Found: A -> Fetch
  | MissingAt: N -> Fetch
  | ErrorString: string -> Fetch
.
\end{lstlisting}

To see why carrying along {\em where} a fetching operation failed is useful,
let us consider how a disk image is constructed. The Coq language does not
provide a mechanism for file access, so we cannot write a program within it to
populate our Disk image. The definitions and functions that define what data
is needed are only available within Coq, however, so while we could use Python
or Scala or any general-purpose language to retrieve necessary bytes, those
languages would not know which bytes were actually necessary.

Luckily, Coq has an {\tt Extraction} command, which allows a Coq function/data
structure to be exported to Ocaml, Haskell, or Scheme. The extraction process
can include all necessary types and referenced terms to build the requested
expression. With this ability, we can use the definitions created in Coq in
concert with file access built into those languages to construct an entire
disk structure. We would simply extract a function which requires the disk
image, feed it an empty {\tt Disk}, see where ({\tt MissingAt}) the function
failed, retrieve that byte from disk, add it to the {\tt Disk} structure, and
repeat until the function was successful.

\subsection{Assumptions and Gunzipping}
\label{subsec:gunzip}

As we just saw, Coq's proof-oriented nature has left the language with several
limitations that need to be worked around in our implementation. Coq programs
cannot, for example, read bytes from a disk image; they cannot (for our
intents) call external programs to transform data; they cannot even
instantiate a list of tens of thousands of values in memory. We work around
these limitations by making {\bf assumptions} about our environment (with the
suffix {\tt \_a}) which entail only the relevant pieces of data needed for our
proofs. As discussed above, we won't read a full disk image; instead, we will
generate a {\it sparse} map to represent that disk, including only the offsets
we care about.

Assumptions are also used to account for transformations that would normally
be performed by a trusted, external program. We cannot, for instance, run
``gunzip'', nor do we want to try to implement that algorithm within Coq.
Instead, we require an assumption be made that encompasses that activity. Our
proofs cannot validate that {\tt honeynet\_disk\_a} or {\tt gunzip\_a}
accurately represent values on the disk or an unzipping function -- they
instead trust those assumptions to be accurate and rely on users (or programs;
see Section \ref{sec:future}) for verification.

The {\tt gunzip\_a} assumption represents gzip's unzipping/deflating
operation, converting a compressed, gzip file into the corresponding,
uncompressed archive file. As we want to avoid implementing this function
properly, we will instead provide a function which throws away its input {\tt
File} and returns a constant {\tt File} which has been pre-populated with the
relevant bytes. We generate the data needed for this disk by using the {\it
Sleuth Kit}'s {\tt icat} program to pull inode {\tt 23}'s contents. We then
ran {\tt gunzip} on the resulting {\tt tgz} file, and then pulled the relevant
bytes (i.e. those necessary for the tar file header, see Section
\ref{sec:tar}) into a {\tt FMapAVL}. The latter step could be automated using
{\tt Fetch} as described in the previous section.

The resulting map is wrapped in a {\tt ByteData} and then a {\tt File} with a
"Mock" file system. As mentioned when discussing {\tt fetchByte}, this causes
byte retrieval operations to defer to the {\tt ByteData}, so we have
effectively generated by {\tt File} from ``thin-air''.

\begin{lstlisting}
Definition gunzipped_23 : Map_N_Byte := 
[ 
  0 |-> ("108":Byte), 
  1 |-> ("097":Byte), 
  2 |-> ("115":Byte), 
  ...
].

Definition gunzip_a := (fun (input: File) => 
  let asDisk := Disk_of_Map_N_Byte gunzipped_23 in
  (mkFile (FileIds.MockId asDisk)
          1454080 (* uncompressed file size *)
          input.(deleted) 
          (* Fields not used; ignore them *)
          None None None None)).
\end{lstlisting}

We pause now to emphasize that, from Coq's perspective, there is no
relationship between the constructed disk image and {\tt gunzipped\_23}. We
trust the proof results because we know the {\em lineage} of the data used to
create {\tt gunzipped\_23}. Encapsulating that lineage programmatically is an
area of future research, discussed in Section \ref{sec:future}.

\subsection{Proof By Reflection}
\label{subsec:reflection}

Each of the definitions we have seen so far lives within Coq's {\tt Prop}
realm. For properties to be proven valid, Coq's proof tactics are required,
describing how assumptions can be transformed to imply the conclusion. This
lies in contrast to Coq's boolean type, which can be {\bf computed}, meaning
boolean expressions can be reduced to a single value. To make the distinction
clear, let us consider two analogous goals involving a simple reduction.

\begin{lstlisting}
Lemma reduce_prop:
  forall (P Q R :Prop),
  Q -> R -> ~P ->
    (Q \/ ~R) /\ (P \/ (~P /\ R /\ Q)).
Proof.
  intros.
  split.
    (* Q \/ ~R *)
    left. assumption.
    (* P /\ (~P /\ R /\ Q) *)
    right. 
    split.
      (* ~ P *) assumption.
    split.
      (* R *) assumption.
      (* Q *) assumption.
Qed.

Lemma reduce_bool:
  forall (p q r :bool),
  q = true -> r = true -> p = false ->
    (q || (negb r)) && (p || ((negb p) && r && q)) = true.
Proof.
  intros. rewrite H. rewrite H0. rewrite H1.
  compute.
  reflexivity.
Qed.
\end{lstlisting}

The latter is largely a matter of plugging in values followed by the {\tt
compute} tactic. Since it deals only with boolean logic, which can be reduced
automatically, we do not need to tease it apart as we do with the {\tt Prop}
version. This makes sense, as {\tt Prop} represents more than just boolean
values; how could we {\em compute} that a property held for all integers, or
that there {\em existed} a certain type of value?

Not all properties are this complex, however, particularly in our problem
domain. We are largely proving that byte values are equivalent, meaning it
would be awfully nice to rely on boolean algebra in our proofs. We do not want
the definitions to be entirely composed on booleans, however, as boolean
expressions have significantly less expressive power. If we were able to prove
that a boolean reduction {\em implied} a property, however, we could prove
that property simply by running {\tt compute}. This is the essence of proof by
reflection. 

As an example, consider our {\tt isJpeg} property. We have also written an
equivalent, boolean version, {\tt isJpeg\_compute} and a lemma, {\tt
isJpeg\_reflection} which shows how the boolean version implies the property.
Note that {\tt Byte.feqb} is a boolean form of equality over {\tt Fetch
Byte}s.

\begin{lstlisting}
Definition isJpeg (file: File) (disk: Disk) :=
     file @[  0 | disk ] = Found "255"
  /\ file @[  1 | disk ] = Found "216"
  /\ file @[- 2 | disk ] = Found "255"
  /\ file @[- 1 | disk ] = Found "217".

Definition isJpeg_compute (file: File) (disk: Disk) :=
     Byte.feqb (file @[  0 | disk ]) (Found "255")
  && Byte.feqb (file @[  1 | disk ]) (Found "216")
  && Byte.feqb (file @[- 2 | disk ]) (Found "255")
  && Byte.feqb (file @[- 1 | disk ]) (Found "217").

Lemma isJpeg_reflection (file: File) (disk: Disk) :
  isJpeg_compute file disk = true -> isJpeg file disk.
\end{lstlisting}

Using proof by reflection (as we do with \\{\tt borland\_reflection}) not only
reduces the number of proof tactics necessary, it also reduces the size of the
so-called ``proof term.'' The proof term is the ultimate evidence that a
property holds; it is what can be shipped around and verified in our
hypothetical court cases. If the size of this proof term grew proportional to
the size of the disk image, the terms would quickly become unmanageable. Plausible use of these techniques would likely mean applying properties over
thousand (if not millions) of files; only if the proof terms remain a constant
size can they be useful at this scale. Proof by reflection accomplishes that
goal.

\subsection{Computed Structures+}

\section{Ext2 Structures and Computations+}

In proving that a file was present on an Ext2 disk image, we deferred to the
simple interface of {\tt Ext2.findAndParseFile}; similarly, when determining
individual bytes of a file, we relied on the {\tt Ext2.fileByte} function.
Unfortunately, these interfaces hide a great deal of complexity present within
Ext2 disks. As a proof of concept, we next provide implementation details for
this function; it should be easy to see how this research could be extended to
include additional file systems so long as a similar interface can be
provided.

\subsection{A Note on Functional Idioms}
\label{subsec:functionalext2}

While computing, we will make use of several idioms from functional
programming.  In particular, we make heavy use of the ``option'' pattern; due
to the sparse map that we use to represent the disk, the parsing code does not
know whether requested bytes will be present. To account for this fact, each
attempt to read returns an {\em option} of the result. We use the {\tt Fetch}
type (discussed in \ref{subsec:fetch}) to represent these options.

The benefit of wrapping the value in the monadic {\tt Fetch} is that we can
continue computation without knowing (in advance) whether a sub-computation
was successful or not. {\tt Fetch}s make error handling ``percolate up'' in
that functions which produce them can be chained such that {\em any} error
in a component causes the entire computation to result in an error. To get to
this point, we define two functions, indicated by the infix notation {\tt
\_fmap\_} and {\tt \_fflatmap\_} (the prepending `f' indicates that the {\tt
Fetch} type is used rather than Coq's built-in {\tt Exc}. The function
signatures for each should aid their explanation.

\begin{lstlisting}
Definition fetch_map {A B: Type} (opt: @Fetch A) (fn: A -> B)
  : Fetch B.

Definition fetch_flatmap {A B: Type} (opt: @Fetch A) 
  (fn: A -> @Fetch B)
  : @Fetch B.
\end{lstlisting}

The role of {\tt \_fmap\_} is to transform the contents of a {\tt Fetch}, if
present. If the {\tt Fetch} contains an error, map has no effect.  {\tt
\_fflatmap\_} similarly does not affect such {\tt Fetch}s. However, if a value
is present (indicated by {\tt Found}), the provided function ({\tt fn}) is
applied and the {\tt Fetch} is replaced with that function's result. In other
words, {\tt \_fflatmap\_} is like applying a {\tt \_fmap\_} and then stripping
the outer {\tt Fetch}. These functions allow sequences like

\begin{lstlisting}
  (file @[ 0 | disk ]) _fflatmap_ (fun byte0 =>
  (file @[ 1 | disk ]) _fflatmap_ (fun byte1 =>
  (file @[ 2 | disk ]) _fmap_ (fun byte2 =>
    (byte0, byte1, byte2)
  )))
\end{lstlisting}

which can return an {\tt ErrorString}; a {\tt MissingAt} with 0, 1, or 2; or
{\tt Found (Byte, Byte, Byte)} (all three bytes present.) If the first byte
was not present, the outer-most {\tt \_fflatmap\_} would not have executed the
inner function. Similarly, if the second byte were not present, the function
including {\tt \_fmap\_} would not be ran, and if the third byte were
unavailable, the function with the parameter {\tt byte2} would not be
evaluated.

We have rushed through these concepts as we assume the reader has some
background in functional programming. For a more thorough (yet still brief and
practical) introduction, see Wamper \& Miller\cite{scala} or Minsky,
Madhavapeddy, \& Hickey\cite{ocaml}.

\subsection{findAndParseFile}

To explain our two functions, we will need to understand how Ext2 file systems
are laid out on disk. We will proceed by pealing off each layer of the calls
and review the data structures created.

At the outermost layer of {\tt findAndParseFile}, we have a function which,
when given a disk and Ext2 Inode identifier, returns a {\tt Fetch File}. As we
discussed earlier, a {\tt File} is composed of a file-system-specific
identifier, file size, deletion status, and a few time-related values.  These
fields require we first parse out the {\tt SuperBlock} of the disk as well as
the {\tt GroupDescriptor} and {\tt Inode} associated with our Inode index (the
unique file identifier for the Ext2 file system).

To understand what these structures represent, let us discuss Ext2. At its
core, the file system is composed of a sequence of equally-sized chunks of
bytes (called ``blocks''.) Files are not necessarily stored on contiguous (or
even in-order) blocks; their data may be parcelled throughout the disk image
(as we will describe when discussing Inodes, below.) Each file is referenced
by a single ``Inode'' structure, which keeps track of the file's data
locations as well as access times, file size, and other meta data. 

Inodes are collected into ``groups'', which have meta data stored in a ``Group
Descriptor''. This descriptor includes collective information about Inodes
(such as which are allocated,) as well as provides a mechanism to segment the
administration of Inodes. We can easily compute to which group a particular
Inode index belongs by dividing it by the number of inodes per group (a
file-system-wide setting found in the SuperBlock.) We can also compute the
Inode's position within that group by taking the remainder of this division
(i.e. by applying mod.) It's important to note for both of these operations
that Inode indices are one-indexed; this will lead to some additions and
subtractions by one throughout our code to convert between zero- (with which
it is easier to compute) and one-based indices.

Meta data about the file system as a whole is stored in a special block known
as the ``SuperBlock'', which lives at a predictable position on the disk. The
SuperBlock contains information such as how large each block is, where the
collection of GroupDescriptors starts, and the number of Inodes per group.
While we do not use this fact, the SuperBlock is usually stored redundantly on
the disk, meaning we could verify its values with some of the redundant
copies.

Returning to our parsing efforts we must note that each attempt to pull out a
SuperBlock, GroupDescriptor, etc. may fail, and if this occurs, we want the
entire File parsing function to propagate the failure. Here we will use the
{\tt \_fflatmap\_} approach described above, treating the computation as a
monad.

\begin{lstlisting}
Definition findAndParseFile (disk: Disk) (inodeIndex: N) 
  : @Fetch File :=
  (findAndParseSuperBlock disk) _fflatmap_ (fun superblock =>
  let groupId := ((inodeIndex - 1) (* One-indexed *)
                  / superblock.(inodesPerGroup)) in
  let inodeIndexInGroup := 
    (inodeIndex - 1) mod superblock.(inodesPerGroup) in
  (findAndParseGroupDescriptor disk superblock groupId) 
    _fflatmap_ (fun groupdesc =>
  (findAndParseInode disk superblock groupdesc inodeIndex) 
    __flatmap_ (fun inode =>
  (parseDeleted disk superblock groupdesc inodeIndex) 
    _fmap_ (fun deleted =>
    mkFile
      (FileIds.Ext2Id (value inodeIndex))
      inode.(size)
      deleted
      (value inode.(atime))
      (value inode.(mtime))
      (value inode.(ctime))
      (value inode.(dtime))
  )))).
\end{lstlisting}

Stripping off this layer, we next take a look at each of the composing
computations, {\tt findAndParseSuperBlock}, \\
{\tt findAndParseGroupDescriptor}, {\tt findAndParseInode}, and {\tt
parseDeleted}.

\subsection{findAndParseSuperBlock}

Regardless of block size, the first SuperBlock can be found starting at the
1024th byte. Below this position is the boot sector, executable code that
loads prior to the main operating system (think boot loaders like LILO, GRUB,
and MBR.) While the size of the SuperBlock depends on the revision of Ext, the
spec is largely compatible, so we will use the SuperBlock structure described
by one of Ext2's original authors, David Poirier\cite{non-gnu}. This spec
cares only about the first 264 or so bytes of the block (regardless of its
full size.)

Finding and parsing the SuperBlock, then, amounts to jumping to offset 1024 on
the disk and plugging in the read values into a SuperBlock structure. To make
our lives a tad easier, we will use a {\tt shift} function, which effectively
shifts the beginning of a {\tt Disk} by serving as a layer of indirection when
requesting bytes. This wrapper allows us to parse as if the 1024th index were
at position zero.

\begin{lstlisting}
Definition shift (bytes: ByteData) (shiftAmount index: N)
  : @Fetch Byte :=
  bytes (shiftAmount + index).
\end{lstlisting}

The values we need to store to construct a SuperBlock are largely encoded as
little endian, 4-byte integers. To parse this sequence of bytes into Coq's
{\tt N} type, we will make frequent use of an unsigned conversion function,
{\tt seq\_lendu}. This function simply reads in a sequence of bytes from the
disk starting at a position (as given by the second parameter) and running for
a specific length (as given by the third,) and converts that into an integer
based on little endian semantics. 

With that, we can show our SuperBlock-parsing function. Note that in this code
sample, we omit most of the SuperBlock's fields (there are roughly 45) as
their addition should be clear.

\begin{lstlisting}
Definition findAndParseSuperBlock (disk: Disk)
  : @Fetch SuperBlock :=
  let disk := (shift disk 1024) in
  (seq_lendu disk 0 4) _fflatmap_ (fun inodesCount =>
  (seq_lendu disk 4 4) _fflatmap_ (fun blocksCount =>
  (* ... Additional fields omitted ... *)
  (seq_lendu disk 260 4) _fmap_ (fun firstMetaBg =>
    mkSuperBlock
      inodesCount
      blocksCount
      (* ... Additional fields omitted ... *)
      firstMetaBg
  ))))))))))))))))))))))))))))))))))))))))))))).
\end{lstlisting}

While we parse out virtually all of the fields of a SuperBlock, we will need
only a handful. {\tt inodesCount} provides an upper bound for inode indices
(which we use to validate that a given inode exists.) {\tt inodesPerGroup}
will appear several times in this paper; the field indicates the number of
inodes assigned to each GroupDescriptor. That field therefore provides a way
to determine which GroupDescriptor is needed for a particular inode index. The
{\tt logBlockSize} field is also of interest, as we will use it to determine
the number of bytes each block spans on disk. This is encoded using the
logarithmic scale, but we provide a simple function to convert to the base-10
number of bytes.

\begin{lstlisting}
Definition blockSize (superblock: SuperBlock) := 
  N.shiftl 1024 superblock.(logBlockSize).
\end{lstlisting}

\subsection{Block Addresses}

As mentioned earlier, the Ext2 file system breaks the disk into ``blocks'' for
reference purposes. Due to locality of reference, in practice, the need to
retrieve a single byte from a disk is very rare. Instead, data is most often
retrieved in sequential chunks, which maps well to the concept of data blocks.
Depending on cache size, whole blocks are read at a time. These blocks are
identified by their one-indexed ``block address'', signified by the type, {\tt
BA}. 

To find the initial byte of a block based on its address alone, we need to
first find the size of each block, which is encoded in the SuperBlock as
described above. With that, we can treat the disk as an array of blocks and
simply jump to the relevant byte position.

\begin{lstlisting}
Definition BA := N.

Definition ba2Offset (superblock: SuperBlock) (blockAddress: BA)
  := (blockSize superblock) * blockAddress.
\end{lstlisting}

\subsection{findAndParseGroupDescriptor}
An array of GroupDescriptors can be found in the block following that which
contains the SuperBlock. If the block size is greater than 1024 (and hence,
the SuperBlock is part of block zero,) the GroupDescriptors begin at block
one. Otherwise, the SuperBlock composes all of block one, so the
GroupDescriptors can be found at block two.

GroupDescriptors will be represented by a structure similar to SuperBlocks,
though with far fewer fields. GroupDescriptors have 32 allocated bytes, yet
only 20 are used. With this fact and knowledge of where the group descriptor
array starts, we can jump to a particular GroupDescriptor by multiplying the
structure's size by the index we seek. As with the SuperBlock, we will shift
the disk to aid our parsing efforts; unlike the SuperBlock, GroupDescriptors
are small enough that we will include their full parsing code here.

\begin{lstlisting}
Definition findAndParseGroupDescriptor 
  (disk: Disk) (superblock: SuperBlock) (groupId: N)
  : @Fetch GroupDescriptor :=
  let groupBlockArrayBA := if (1024 <? blockSize superblock)
    then 1 else 2 in
  let groupBlockArrayOffset := 
    ba2Offset superblock groupBlockArrayBA in
  let descriptorOffset := 32 * groupId  in
  let disk := (shift disk (groupBlockArrayOffset 
                           + descriptorOffset)) in
  (seq_lendu disk 0 4) _fflatmap_ (fun blockBitmap =>
  (seq_lendu disk 4 4) _fflatmap_ (fun inodeBitmap =>
  (seq_lendu disk 8 4) _fflatmap_ (fun inodeTable =>
  (seq_lendu disk 12 2) _fflatmap_ (fun gdFreeBlocksCount =>
  (seq_lendu disk 14 2) _fflatmap_ (fun gdFreeInodesCount =>
  (seq_lendu disk 16 2) _fmap_ (fun usedDirsCount =>
    mkGroupDescriptor
      blockBitmap
      inodeBitmap
      inodeTable
      gdFreeBlocksCount
      gfFreeInodesCount
      usedDirsCount
  )))))).
\end{lstlisting}

Two of the fields from this structure will be particularly useful, {\tt
inodeBitmap} and the {\tt inodeTable}. Both contain a block address pointing
to the start of a sequence of inode-related data. The former is simply a bit
sequence where each bit represents whether or not the corresponding Inode is
allocated; this will be of great use when we are calculating deletion status.
The latter points to an array of Inode structures, as described in the next
section.

\subsection{findAndParseInode}

Ext2 tracks meta information about specific files through ``Inode''
structures. One such data structure exists for each file and contains copious
information relevant to our interests, including creation time, deletion time,
and references to the data blocks which make up this file. As each file has a
unique Inode within the Inode array, we can refer to files by their ``Inode
Index'', which we do throughout this paper. As mentioned before, Inode indices
are one-indexed, so the first Inode is associated with index 1.

GroupDescriptors contain a reference to the block address associated with the
start of the Inode array. From there, we can pin point the beginning of the
relevant Inode structure by calculating the Inode's position within the block
group the same way we calculated GroupDescriptors -- we know that Inodes are
128-byte data structures.  Add in a one-based offset and a check that the
requested Inode is valid and you have the {\tt findAndParseInode} function.

\begin{lstlisting}
Definition findAndParseInode (disk: Disk) 
  (superblock: SuperBlock) (groupdesc: GroupDescriptor)
  (inodeIndex: N) : @Fetch Inode :=
  (* Check for valid Inode *)
  if (superblock.(inodesCount) <=? inodeIndex)
  then ErrorString "Invalid inode index"
  else
    (* Inode Table is 1-indexed *)
    let inodeIndexInTable := 
      ((inodeIndex - 1) mod superblock.(inodesPerGroup)) in
    let inodePos := (ba2Offset superblock
                               groupdesc.(inodeTable))
                     + (inodeIndexInTable * 128) in
    let disk := (shift disk inodePos) in
    (seq_lendu disk 0 2) _fflatmap_ (fun mode =>
    (* ... Additional fields omitted ... *)
    (seq_lendu disk 40 4) _fflatmap_ (fun directBlock1 =>
    (* ... Additional direct blocks omitted *)
    (seq_lendu disk 84 4) _fflatmap_ (fun directBlock12 =>
    (seq_lendu disk 88 4) _fflatmap_ (fun indirectBlock =>
    (seq_lendu disk 92 4) _fflatmap_ (fun 
                                      doubleIndirectBlock =>
    (seq_lendu disk 96 4) _fflatmap_ (fun 
                                      tripleIndirectBlock =>
    (* ... Additional fields omitted ... *)
    (seq_lendu disk 116 4) _fmap_ (fun osd2 =>
      mkInode
        mode
        (* ... Additional fields omitted ... *)
        (directBlock1 :: directBlock2 :: directBlock3
          :: directBlock4 :: directBlock5 :: directBlock6
          :: directBlock7 :: directBlock8 :: directBlock9
          :: directBlock10 :: directBlock11 :: directBlock12
          :: indirectBlock :: doubleIndirectBlock
          :: tripleIndirectBlock :: nil)
        (* ... Additional fields omitted ... *)
        osd2
    )))))))))))))))))))))))))))))))).
\end{lstlisting}

This code sample highlights the twelve ``direct block'' pointers and three
indirection pointers, which we combine into a list, making the {\tt block}
field (keeping with Poirier's conventions.) Ext2 was designed with a trade-off
in mind; quick access to file data generally runs counter to the desire to
store very large files.  The solution the file system employs is to allow the
first twelve data blocks to be directly addressable from the Inode. Slightly
larger files can be addressed through an indirection pointer, which points to
a block which in turn points to several blocks of data. Even larger files can
be addressed through a double-indirection pointer, which points to a block
which points to other blocks which each contain references to additional data
blocks. There is also a third level of indirection, allowing Ext2 to address
up to roughly sixteen million data blocks.

\subsection{parseDeleted}
With the Inode and its index, we know the File's file system identifier, size,
and modification-access-creation-deletion (MAC) times.  We next turn to the
field indicating whether or not the file is marked as deleted, or
``unallocated.'' To find the allocation status of an Inode, we will need to
find the associated allocation bitmap. As mentioned above, the bitmap
associated with a particular Inode is indicated in that Inode's
GroupDescriptor. As we are only reading whole bytes at a time, we will read
the byte that contains the allocation bit for the Inode we care about and then
test the bit (1, allocated, translates to true; 0 to false) within that byte.

Ultimately, this means we will find the start of the allocation bitmap from
the GroupDescriptor, jump to the byte within that bitmap, read it, and test
it. As noted before, we need to be weary of the fact that Inodes are
one-indexed.

\begin{lstlisting}
Definition parseDeleted (disk: Disk) (superblock: SuperBlock)
  (groupDesc: GroupDescriptor) (inodeIndex: N) : @Fetch bool :=
  let inodeIndexInGroup := 
    (* 1-Indexed *)
    (inodeIndex - 1) mod superblock.(inodesPerGroup) in
  let bitmapStart := 
    ba2Offset superblock groupDesc.(inodeBitmap) in
  (* Fetch the allocation byte for this inode *)
  (disk (bitmapStart + (inodeIndexInGroup / 8))) 
    _fmap_ (fun allocationByte =>
    (* The bit associated with this inode is 0 *)
    match (allocationByte, inodeIndexInGroup mod 8) with
    | (Ascii b _ _ _ _ _ _ _, 0) => negb b
    | (Ascii _ b _ _ _ _ _ _, 1) => negb b
    | (Ascii _ _ b _ _ _ _ _, 2) => negb b
    | (Ascii _ _ _ b _ _ _ _, 3) => negb b
    | (Ascii _ _ _ _ b _ _ _, 4) => negb b
    | (Ascii _ _ _ _ _ b _ _, 5) => negb b
    | (Ascii _ _ _ _ _ _ b _, 6) => negb b
    | (Ascii _ _ _ _ _ _ _ b, 7) => negb b
    | _ => false  (* should never be reached *)
    end
  ).
\end{lstlisting}

\subsubsection{fetchInodeByte}

We need only one more attribute to create our File structure. File objects
have a method which, given an arbitrary offset, returns the associated byte
within the File, effectively linearizing byte access. This abstraction is
quite necessary as each File system will have a different method of accessing
the File bytes.

Remembering the structure of Ext2 Inodes, we know that determining which data
block a given byte of a file is located in may require walking one or more
levels of indirection. As this is a recursive operation (depending on the
level of indirection,) we next describe a fixpoint which will pull out the
block address associated with a given byte. This function checks if it has
reached the final level of indirection, as indicated by the {\tt O} (Coq's
zero) case; if so, it knows to grab the correct block address (a four byte
value) from the block of pointers.

\begin{lstlisting}
Fixpoint walkIndirection (disk: Disk) (superblock: SuperBlock)
  (blockNumber indirectionPos: Z) (indirectionLevel: nat) 
  : Exc Z :=
  match indirectionLevel with
  | O => 
    let bytePosition := (indirectionPos + 4 * blockNumber) in
    (seq_lendu disk bytePosition 4)
  ...
\end{lstlisting}

If the function has not reached its base case (as indicated by {\tt S}, the
``successor of,'') it determines which of its block addresses to follow,
reduces the block number requested accordingly, and recursively tries again.
To select the correct block address to recursively follow, we must determine
how many blocks each indirection pointer is responsible for ({\tt
unitSizeInBlocks},) and select the index of the block address within the array
accordingly ({\tt nextBlockIndex}.) Using that, and the fact that each address
is four bytes large, we jump to the correct offset within the pointer array,
read those four bytes, and recurse accordingly.

\begin{lstlisting}
Fixpoint walkIndirection (disk: Disk) (superblock: SuperBlock)
  ...
  | S nextIndirectionLevel =>
    (* Type conversion *)
    let exponent := Z.of_nat indirectionLevel in
    let unitSizeInBlocks := 
      ((blockSize superblock) ^ exponent) / (4 ^ exponent) in
    let nextBlockIndex := blockNumber / unitSizeInBlocks in
    let nextBytePosition := 
      indirectionPos + 4 * nextBlockIndex in
    (seq_lendu disk nextBytePosition 4) 
      _flatmap_ (fun nextBlockBA =>
      walkIndirection disk superblock 
                      (blockNumber mod unitSizeInBlocks)
                      (ba2Offset superblock nextBlockBA)
                      nextIndirectionLevel
      )
  end.
\end{lstlisting}

Building on top of this fixpoint, we can define a single function that, when
given a Disk, SuperBlock, Inode and offset, returns the byte within the
associated file at that offset. We destruct the function in to three parts.
First, we check if the byte requested is within the file; if not, we
immediately {\tt error}.

\begin{lstlisting}
Definition fetchInodeByte (disk: Disk) (superblock: SuperBlock)
  (inode: Inode) (bytePos: Z): Exc Z :=
  if inode.(size) <=? bytePos then 
    error
  else 
    ...
\end{lstlisting}

Next, we determine the block address using the recursive function described
above. We begin by noting bounds on which data blocks are addressable directly
and at the levels of indirection.

\begin{lstlisting}
Definition fetchInodeByte (disk: Disk) (superblock: SuperBlock)
    ...
    let blockSize := (blockSize superblock) in
    let blockNumInFile := bytePos / blockSize in
    let directAddressable := 12 in
    let indirect1Addressable := blockSize / 4 in
    let indirect2Addressable := (blockSize * blockSize) / 16 in
    ...
\end{lstlisting}

How we attain the block address of the data depends on the byte requested. If
the byte is in the first 12 data blocks, we have immediate access to the block
address from the Inode's {\tt block} list.

\begin{lstlisting}
Definition fetchInodeByte (disk: Disk) (superblock: SuperBlock)
    ...
    let blockAddress :=
    (if blockNumInFile <? directAddressable then
      nth_error inode.(block) (Z.to_nat blockNumInFile)
    ...
\end{lstlisting}

If, instead, the block falls within the range of the first indirection block,
we need to call our {\tt walkIndirection} function.

\begin{lstlisting}
Definition fetchInodeByte (disk: Disk) (superblock: SuperBlock)
    ...
     else if blockNumInFile <? directAddressable
                               + indirect1Addressable then
      (nth_error inode.(block) 12) 
        _flatmap_ (fun indirectBlock =>
        walkIndirection disk superblock
          (blockNumInFile - directAddressable)
          (ba2Offset superblock indirectBlock) 
          O
        )
    ...
\end{lstlisting}

If the block falls within the range of the double indirection block, we call
{\tt walkIndirection} with an additional level. Everything else falls to the
triple indirection block.

\begin{lstlisting}
Definition fetchInodeByte (disk: Disk) (superblock: SuperBlock)
    ...
    else if blockNumInFile <? directAddressable
                              + indirect1Addressable
                              + indirect2Addressable then
      (nth_error inode.(block) 13) 
        _flatmap_ (fun doubleIndirectBlock =>
        walkIndirection disk superblock 
          (blockNumInFile - directAddressable
                          - indirect1Addressable)
          (ba2Offset superblock doubleIndirectBlock)
          (S O)
        )
    else 
      (nth_error inode.(block) 14) 
        _flatmap_ (fun tripleIndirectBlock =>
        walkIndirection disk superblock 
          (blockNumInFile - directAddressable
                          - indirect1Addressable
                          - indirect2Addressable)
          (ba2Offset superblock tripleIndirectBlock) 
          (S (S O))
      )
    ) in
    ...
\end{lstlisting}

Finally, once we have the block address for the data block, we need to jump to
the actual byte requested. We determine this simply by taking the byte
position requested modulo the size of each data block.

\begin{lstlisting}
Definition fetchInodeByte (disk: Disk) (superblock: SuperBlock)
    ...
    let blockAddress := (
      ...
    ) in
    blockAddress _flatmap_ (fun blockAddress =>
      disk ((ba2Offset superblock blockAddress) 
            + (bytePos mod blockSize))
    ).
\end{lstlisting}

With this function, we have completed each of the components needed to
represent a file structure from an Ext2 disk. Putting them together (via {\tt
mkFile}) allows us to use that file as the evidence needed for much of {\tt
borland\_honeynet\_file}. In particularly, we know how to prove that the file
{\tt isOnDisk} (if we can provide an Inode Index that matches it,) that it the
file {\tt isDeleted} (if the deleted flag is set,) and that the file {\tt
isGzip} (if the first three bytes are those of a Gzip.)

All that's left is to show that the decompressed tar \\
{\tt looksLikeRootkit}.

\begin{lstlisting}
Lemma borland_honeynet_file:
  ...
  /\ Tar.looksLikeRootkit (gunzip_a file).
\end{lstlisting}


\section{Lemmas, Computations, Definitions; Oh My!+}

We have now come across several core concepts of this research, and it is easy
to see how they might be confused. Before we continue, let us solidify our
understanding of each category.

We are most familiar with {\bf definitions}, which provide a name for a common
understanding of a forensics concept. For example, we provided definitions for
{\tt isGzip}, {\tt looksLikeRootkit}, etc. While one of the wider goals of
this research is to provide a set of such common definitions, this paper only
describes several which are relevant to the Honeynet example. Other
definitions might include ``web page access'' (e.g. by inspecting browser
history,) ``in contact with'' (e.g. there exists records of email
communication,) and ``last time logged in.'' Note that definitions are, at
their core, the choice of their creators; forming a consensus on a definition
is the only way it may provide authority. This also means that the possibility
for multiple definitions is present (as we have shown with our two definitions
for file existence.)

Definitions are most often built by combining aspects of various abstract {\bf
data structures}, ``universal'' representations of data-related concepts
within forensics. We have seen these used to represent files, and we will see
them represent various Ext2 file system structures, as well as events in a
reconstructed timeline. These structures serve as a way of separating
particular parsing computations (which build the structures) from definitions
involving the structures. While not complete, this division allows us to
reason about our definitions using only the data representations, not the
parsing algorithms. It should be relatively simple for someone to write a
different parser which produces the same structures without affecting many of
the definitions.


The Honeynet competition we have described required researchers provide
``{\bf evidence}'' or ``{\bf proof}'' that the disk had been compromised by a
rootkit. This required that the researchers both define what acceptable
evidence would look like as well as provide that type of evidence for the
attacked server. This led each researcher to give different forms of evidence
depending on what that researcher found to be acceptable. The types of
evidence provided are codified in our definitions (as described above,) but
the evidence they gave maps closer to a Coq {\bf lemma} or proof. The evidence
will be in the form of a proof term, which the author of a proof can build up
using Coq's tactic system. Given a proof that the disk satisfies the
definitions provided, it is only up to a reviewer to determine whether or not
the {\em definitions} are satisfactory. We provide a list of tactics that
generate the proof term for each of our lemmas in the source linked to in our
appendix, and we will expand the implications of this topic further, in
section \ref{sec:unknown}.

Finally, the code written for this research is sprinkled with {\bf utility
functions} to perform (mostly) simple transformations. {\tt
trimFileNamePrefix} and {\tt ascii2Bytes} are two such examples which make
explaining (and hopefully, understanding) the code samples easier. These
functions are not necessary for the definitions, proofs, etc. where they
appear; they simply make the code more terse.

\section{Computing Our First Lemma*}

Now that we have introduced these concepts, let's step through our {\tt
borland\_honeynet\_file} proof, which will touch on each.


\subsection{Parsing a File via INode}

Our evidence relies on the {\it existence} of a {\tt File} which is on disk,
deleted, etc., so our proof for this evidence need only provide such a {\tt
File}. Hypothetically, this should be easy, as we can just pull the file from
the disk image directly. We therefore use a parsing function (rather, a series
of functions) to retrieve this file and provide it as demonstrative proof.
Ultimately, we will need to call

\begin{lstlisting}
Ext2.findAndParseFile honeynet_image_a 23
\end{lstlisting}

To be confident in this function's results, however, we will need to
understand how Ext2 file systems are laid out on disk. We will proceed by
pealing off each layer of the call and review the data structures created.

\subsection{Parsing File Names from a Tar}
Once we've been given the unzipped tar file (i.e. from {\tt gunzip\_a}) we
need to parse its structure well enough to pull out the file names it
contains. This is not terribly dissimilar to pulling out SuperBlocks, Inodes,
etc. from a disk, save that here we are working within the context of a single
file. As files are abstracted to the point that they need only provide a
function to access their data, we need not worry about underlying file system
mechanics.

A tar file is composed of a sequence of (header, file content) pairs such that
the header contains meta data about the file (including its name, size, owner,
etc.) and the file data is padded to a multiple of 512 bytes. To retrieve all
of the file names contained within a tar, we will need to read the ``first''
file name from the tar header, read the file size from that header, skip the
file contents and start again with the following header.

\subsubsection{File Size from ASCII Octal}
File size of a file within a tar can be determined by reading the 11 bytes
starting at offset 124 of the tar header. The size is first encoded into
octal, which is then represented as ASCII characters. Encoding in ASCII stems
from the desire to keep the header human readable; the choice of octal is a
bit more dubious, but {\it c'est la vie}. We implement our parsing with a
simple recursive function which reads the next byte from a list and converts
that into the encoded integer value. The value is multiplied by 8 raised to
the length of rest of the list to account for octal order of magnitude. If the
byte does not fall into the range associated with ascii `0' through `7', an
{\tt error} is returned as anything else does not fit our encoding
expectations.

\begin{lstlisting}
Fixpoint fromOctalAscii (bytes: list Z) : Exc Z :=
  match bytes with
  | nil => value 0
  | byte :: tail => match (fromOctalAscii tail) with
    | error => error
    | value rest => if (andb (48 <=? byte) (byte <=? 56))
      then value (rest + ((byte-48)
                          * (8 ^ (Z.of_nat (length tail)))))
      else error (* Invalid character *)
    end
  end.
\end{lstlisting}

\subsubsection{More Functional Idioms}

The first hundred bytes of the header are dedicated to the filename
(regardless of filename length); the filename is null-terminated. To get
there, we will add three new functional idioms: {\tt upto}, {\tt flatten}, and
{\tt takeWhile}. {\tt upto} (or {\tt range}) acts as a simple, infix way to
create a sequence of integers between the provided end points; we skip its
definition as it involves too much Coq-specific inside baseball. Similar to
our need for {\tt \_flatmap\_}, {\tt flatten} takes a list of options and
converts them to a list of the options' contents, skipping any {\tt error}s.
Finally {\tt takeWhile} is a function that takes two parameters, a boolean
predicate and a list of elements. This function returns the longest prefix of
that list such that every element in the prefix satisfies the predicate.

\begin{lstlisting}
Fixpoint flatten {A} (lst: list (Exc A)): list A :=
  match lst with
    | (value head) :: tail => head :: (flatten tail)
    | error :: tail => flatten tail
    | nil => nil
  end.

Fixpoint takeWhile {A} (predicate: A->bool) (lst: list A)
  : list A :=
  match lst with
  | head :: tail => 
    if (predicate head) 
    then head :: (takeWhile predicate tail)
    else nil
  | nil => nil
end.
\end{lstlisting}

\subsubsection{Parsing a Single Header}
Next, we will write a function that can both pull out the first filename from
a tar as well as the first file's contents. Retrieving the file name requires
reading the first hundred bytes of the tar and taking every character until a
null is read. Due to our abstract concept of a file, extracting the first file
from the tar is relatively straight forward. We need only parse the file size
(making use of a sequence reader, {\tt seq\_list},) carry over the deleted
status of the parent tar file, and drop the tar header from the data to create
a File object (we do not provide any of the optional values.)

\begin{lstlisting}
Definition parseFirstFileNameAndFile (tar: File)
  : Exc (ByteString*File) :=
  let firstHundredBytes := map tar.(data) (0 upto 100) in
  let fileName := flatten (takeWhile 
    (fun (byte: Exc Z) => match byte with
      | error =>   false (* stop *)
      | value 0 => false (* stop *)
      | value _ => true  (* continue *)
    end) firstHundredBytes) in
  (seq_list tar.(data) 124 11) 
    _flatmap_ (fun fileSizeList =>
  (fromOctalAscii fileSizeList) _map_ (fun fileSize =>
    (fileName, 
      (mkFile None (* no id in child files*)
              fileSize
              (* Keep the deleted status of the tar *)
              tar.(deleted)  
              (* Header size = 512 *)
              (shift tar.(data) 512)
              (* Carry over fields from parent tar *)
              tar.(lastAccess)
              tar.(lastModification)
              tar.(lastCreated)
              tar.(lastDeleted)
    ))
  )).
\end{lstlisting}

\subsubsection{Recursing Through}
Now that we have a function which fetches the first filename and file from the
tar, we can call it recursively, creating a new version of the tar file (i.e.
minus the first file) with each step. We need to strip off the initial file
header (512 bytes) plus the file size padded to 512 bytes. We won't use a
fixpoint, opting instead for a parameter ({\tt nextCall}) to signify the
recursive call; this is required by {\tt N.peano\_rect}, a library function
which allows for recursive calls over binary numbers (including our {\tt Z}.)
To see the full definition of {\tt Tar.parseFileNames}, please see the source
code.

\begin{lstlisting}
Definition recFileNameFrom (nextCall: File -> list ByteString) 
  (remaining: File) : list ByteString :=
  if (remaining.(fileSize) <=? 0)
    then nil
  else match (parseFirstFileNameAndFile remaining) with
    | error => nil
    | value (fileName, file) =>
        (* Strip the first file out of the tar *)
        (* Round to the nearest 512 *)
        let firstFileSize := (
          if (file.(fileSize) mod 512 =? 0)
          then file.(fileSize) + 512
          else 512 * (2 + (file.(fileSize) / 512))) in
        let trimmedTar := 
          (mkFile None (* No id in child files *)
                  (remaining.(fileSize) - firstFileSize)
                  remaining.(deleted) (* Parent's value *)
                  (shift remaining.(data) firstFileSize)
                  (* Use parent's values *)
                  remaining.(lastAccess)
                  remaining.(lastModification)
                  remaining.(lastCreated)
                  remaining.(lastDeleted)
          ) in
        fileName :: (nextCall trimmedTar)
    end.
\end{lstlisting}

\subsection{Computing Over File Names}
Now that we have pulled the list of file names from the tar file, we want the
ability to prove that two, distinct file names satisfy {\tt systemFile}. As we
discussed before, this means that we want to compare the file name (sans
directory) with an a priori set of known system files. As a proof of concept,
the list provided is slim, but it would be quite simple to extend it.

\begin{lstlisting}
Definition systemFile (fileName: ByteString) :=
  In (trimFileNamePrefix fileName)
     (map ascii2Bytes ("ps" :: "netstat" :: "top" :: "ifconfig" 
                       :: "ssh" :: "rsync" :: "ProcMon.exe"
                       :: nil)).
\end{lstlisting}

The {\tt ascii2Bytes} function simply converts each Coq ASCII character into a
corresponding {\tt Z} to represent it as a byte. We use the functional {\tt
map} idiom to run this function an every element of the list of Coq strings.

\begin{lstlisting}
Fixpoint ascii2Bytes (fileName: string): ByteString :=
  match fileName with
  | EmptyString => nil
  | String char tail => 
    (Z.of_N (N_of_ascii char)) :: (ascii2Bytes tail)
  end.
\end{lstlisting}

We also need to define the {\tt trimFileNamePrefix} function, which we want to
return the string after the last directory delimiter (i.e. after the last `/'
or '$\backslash$'.) To do this, we reuse our {\tt takeWhile} function applied
to the reverse of the file name provided, taking bytes until we hit either `/'
or `$\backslash$'. We then reverse that result to get the final, trimmed file
name.

\begin{lstlisting}
Definition trimFileNamePrefix (fileName: ByteString)
  : ByteString :=
  let reversedName := rev fileName in
  rev (takeWhile 
        (fun (char: Z) => 
          (negb (orb (char =? 47)    (* `/' *)
                     (char =? 92)))) (* `\' *)
        reversedName).
\end{lstlisting}

With that, we have described all of the components needed to compute our
lemma. We leave the details of the proof to our code samples, but at the high
level, one can imagine how proving {\tt borland\_honeynet\_file} need only
require we provide a File (by parsing it from the disk) as evidence. We can
more or less compute each of the properties based on that file.

\section{Timelines as Evidence*}
\label{sec:timelines}

A second type of evidence provided by the Honeynet researchers (particularly
from Jason Lee\cite{lee}) had the form of a timeline of events to explain
what the researchers believed happened to the infected server. A timeline is
simply an ordered sequence of events; examples of events include file
modification, user login, system restart, etc. Timelines are certainly useful
as a form of evidence as they provide a narrative of what took place. They are
also provable artifacts, as a timeline is only sound as long as there is
evidence for each of its events and if the order of those events can be
verified. We consider events to be in the correct sequence if events earlier
are {\tt beforeOrConcurrent} events later in the sequence.

\begin{lstlisting}
Definition isInOrder (timeline: Timeline) :=
  (* Events are in the correct sequence *)
  (forall (index: nat),
    (index < ((length timeline) - 1) )%nat ->
      match (nth_error timeline index, 
             nth_error timeline (index + 1)) with
      | (value lhsEvent, value rhsEvent) => 
        beforeOrConcurrent lhsEvent rhsEvent
      | _ => False
      end).

Definition isSound (timeline: Timeline) (disk: Disk) :=
  (forall (event: Event),
    (* Event is evident from the disk *)
    (In event timeline) -> (foundOn event disk))
  /\ isInOrder timeline.
\end{lstlisting}

\subsection{Events and Their Relations}

We next consider an {\tt Event} type in its various forms. For files, we will
see four events: access, creation, modification, and deletion. We will
represent these events with two parameters, a unix-style timestamp of the
event's execution and a file system identifier (as would be found in a File.)

\begin{lstlisting}
Inductive Event: Type :=
  | FileAccess: Z -> Z -> Event
  | FileModification: Z -> Z -> Event
  | FileCreation: Z -> Z -> Event
  | FileDeletion: Z -> Z -> Event
\end{lstlisting}

While not all conceivable events have a timestamp, those that do make the {\tt
beforeOrConcurrent} definition significantly simpler. To prove that one event
happens {\tt beforeOrConcurrent} another, simply compare the timestamps.

\begin{lstlisting}
Definition timestampOf (event: Event) : Exc Z :=
  match event with
  | FileAccess timestamp _ => value timestamp
  | FileModification timestamp _ => value timestamp
  | FileCreation timestamp _ => value timestamp
  | FileDeletion timestamp _ => value timestamp
  | _ => error.

Definition beforeOrConcurrent (lhs rhs: Event) :=
  match (timestampOf lhs, timestampOf rhs) with
  | (value lhs_time, value rhs_time) => lhs_time <= rhs_time
  | _ => False.
\end{lstlisting}

Note that the {\tt beforeOrConcurrent} relation need not be limited to events
where we have a concrete timestamp. We could extend the definition of an event
to include execution of shell scripts, for example, which have a clear
relative ordering of commands but do not have absolute time.

\subsection{Finding Events and Their Existence}

For a Timeline to be valid, each of the events in the timeline must follow
from the disk image. While conceivable events may not be associated with
files, the four we have defined all require the existence of a file that
confirms their relevance. We simply check that there exists a file on disk
that shares the same file system identifier and MAC time.

\begin{lstlisting}
Definition foundOn (event: Event) (disk: Disk) : Prop :=
  exists (file: File),
    isOnDisk file disk
    /\ (match event with
        | FileAccess timestamp fsId =>
            file.(fileSystemId) = value fsId
            /\ file.(lastAccess) = value timestamp
       | FileModification timestamp fsId =>
            file.(fileSystemId) = value fsId
            /\ file.(lastModification) = value timestamp
       | FileCreation timestamp fsId =>
            file.(fileSystemId) = value fsId
            /\ file.(lastCreated) = value timestamp
       | FileDeletion timestamp fsId =>
            file.(fileSystemId) = value fsId
            /\ file.(lastDeleted) = value timestamp
       end).
\end{lstlisting}

\subsection{Applying to Honeynet}
With all of these definitions we can now see how one would provide evidence
that a particular timeline was valid. Consider Jason Lee's entry\cite{lee}
into the Honeynet contest described before. As part of his evidence, he
provided a sequenced list of inode events (as discovered by ``MACtimes'') and
annotated their significance. We copy several of these events and their
annotations into Coq; in the conversion we lose file name (instead, we use
inode number) and pretty-printed dates (opting for unix time stamps.)

\begin{lstlisting}
Lemma lee_honeynet_file:
  (Timeline.isSound (
    (* Mar 16 01 12:36:48 *)
      (* rootkit lk.tar.gz downloaded *)
        (FileModification 984706608 23)
    (* Mar 16 01 12:44:50 *)
      (* Gunzip and Untar rootkit lk.tar.gz *)
        :: (FileAccess 984707090 23)
      (* change ownership of rootkit files to root.root *)
        :: (FileAccess 984707102 30130)
      (* deletion of original /bin/netstat *)
        :: (FileDeletion 984707102 30188)
      (* insertion of trojan netstat *)
        :: (FileCreation 984707102 2056) 
      (* deletion of original /bin/ps *)
        :: (FileDeletion 984707102 30191)
      (* insertion of trojan ps *)
        :: (FileCreation 984707102 2055) 
      (* deletion of origin /sbin/ifconfig *)
        :: (FileDeletion 984707102 48284)
      (* insertion of trojan ifconfig *)
        :: (FileCreation 984707102 2057) 
    (* Mar 16 01 12:45:03 *)
      (* the copy of service files to /etc *)
        :: (FileAccess 984707103 30131)  
      (* hackers services file copied on top of original *)
        :: (FileCreation 984707103 26121)
    (* Mar 16 01 12:45:05 *)
      (* deletion of rootkit lk.tar.gz *)
        :: (FileDeletion 984707105 23)   
    :: nil)
    honeynet_image_a
  ).
\end{lstlisting}

Seeing as the definition of {\tt Timeline.isSound} is composed of two
computable relations, proving this lemma boils down to a series of
computations. The full list of tactics used to build up the proof is linked
from the appendix.

\section{Are These Definitions Sufficient?*}
\label{sec:sufficient}

In this paper, we have provided several definitions for types of evidence that
would be applicable to forensics researchers. It is important to take a step
back and determine whether or not those definitions provide the evidence we
need to prove that a root kit was installed on the disk in question. 

First, we note that the timeline we provided (and verified) consists only of
modification, file system identifiers, and timestamps. We rely on the
annotations to provide context about which file the identifier is associated
with; if we were to swap these annotations with different file names, we could
construct a completely different scenario. Let us assume, however, that we can
derive a filename from the file system identifier (and can therefore verify
annotations) as this is a more interesting situation.

Making this assumption, are there scenarios where our definitions could be
satisfied without a rootkit being installed? Let's start by ruling out
possibilities. While downloading and installing non-system software satisfies
many of the requirements (perhaps a gzipped-tar that is deleted after
replacing some files,) we can rule this scenario out based on the name of the
files replaced (i.e. non-system software installation wouldn't affect system
utilities.) What about an update to a system program? Here we can rely on the
clause within our ``malicious-looking'' check which requires {\em multiple}
system file names.

The obvious next step would be to ask what would happen if we were performing
a system upgrade, one that would affect multiple system files. Here we might
argue that practical package managers would not include the changes in a
single archive, but that argument is not as strong as we desire. It's
conceivable that a hotfix to a predictable operating system, say OS X, might
come as a single, gzipped tar archive which replaces multiple system files. By
all accounts, this would trigger our definition, even though a rootkit had
not actually been installed.

Here, the leap between our definition and its semantics causes us pain --
scenarios like the one described {\em look} like rootkit installation. System
files are replaced by those found within an archive which was deleted shortly
after installation. We might try to eliminate these false positives by
checking for code signatures or describing rootkit files by a tell-tale
fingerprint, either within the executable's bytes or within its actions. That
said, making this exception would simply move the goal post further down the
field; we could surely find a legitimate counter-example even with the
additional restrictions.

We will never be able to completely remove counter-examples as given enough
iterations, a random number generator would eventually generate a disk that
provides {\em exactly} the data we need. Clearly, no rootkit was installed on
such a constructed disk, yet any definition we could conceive could be
satisfied. Ultimately, the definitions cannot guarantee their semantic
meaning; in aggregate the definitions can only make such meaning more and more
likely.

This, too, is the methodology found within the Honeynet competition; as there
were no canonical definitions for what it means for a rootkit to be installed,
each participant provided his or her own evidence which satisfied his or her
own definition. This led them to describe slightly different attacks, even
though they all shared the same disk image.

We did not set out to provide bullet-proof definitions for concepts such as
deletion, file creation, etc. Instead, we wanted to model the same types of
evidence currently put forth by forensics researchers. If a consensus can be
reached regarding these definitions, we would use it, but at the moment we
model only certain de facto standards.

\section{Tractability*}
The final artifact when proving a lemma is a so-called ``proof term'', a
type-checkable entity which serves as verifiable evidence for the lemma. If
this proof term were to grow with the size of the underlying disk (or files or
file headers, etc.,) it would quickly become unmanageable. Larger disks would
eventually prevent a proof-checker from running.

Instead, the design presented causes a well-developed proof to grow only with
the size of the {\em lemmas}. Regardless of disk size, the proof term can
remain equivalent, reducing the burden for proof-checkers. Further, with a
consistent proof term, we open the possibility of expanding beyond some of
Coq's limitations. As discussed previously, due to Coq's representations,
relevant sections on disk must be limited to a few thousand offsets. However,
we could hypothetically export a proof term generated within Coq on a smaller
disk size and run it through a external proof-checker to handle larger disk
images.

\section{Future Work*}
\label{sec:future}

Anyone not already familiar with Coq's tactic systems would be quite confused
by the steps needed to prove any of these lemmas. On the other hand, those who
are familiar with Coq might find the compute-heavy nature of these definitions
to be outside of their comfort zone. Compound that with the rather large
numbers needed, and it becomes safe to say that very few people would be able
to use this system, and none work in the professional forensics space.

A potential solution appears in two phases. First, these proofs are ripe for
automation. Providing one of a pre-defined set of evidence types and a disk
image should be enough for a script to generate the required assumptions,
evidence, and proof terms. As a second step, we could develop a meta language
or other interface for describing the types of evidence needed and allow a
program to search and generate proofs. This connects with wider work by Radha
Jagadeesan, Corin Pitcher, and James Riely of DePaul University, which
includes efforts to automate forensic methods.

This paper describes only a few of the types of evidence that would be
required for a complete forensics tool. We used a single honeynet challenge to
provide constraints and scope. Obviously, creating additional definitions of
evidence would be a key area for future research. This means both tasks such
as defining additional file systems and describing additional types of events.
Imagine what events would define a user logging in, executing a command, and
then disconnecting. What kind of evidence is needed to prove that a user
frequently visited a certain domain while web browsing?

Finally, the vast majority of this paper has been devoted to computations or
definitions; we have not described many propositional relationships. It would
be worthwhile to prove propositions about these definitions (similar to how we
showed JPEG files cannot be Gzips.) We might prove that a patch applied to a
disk image could restore a deleted file (as studied by Charles Winebrinner,)
that deletion events imply the associated file is deleted, that files can
exist which are not addressable, or any number of other proofs and lemmas.
These would then make proofs about particular disk images even easier to
apply.

\acks
First, I wish to thank my colleagues at DePaul, including Malik Aldubayan,
Iana Boneva, Christina Ionides, Daria Manukian, Matthew McDonald, and Charles
Winebrinner for their ideas and community. I would also like to thank my
professors, Radha Jagadeesan, Corin Pitcher, and James Riely for their
feedback and insights. Most notably, working with my advisor, Corin, has been
a great pleasure. Most of the code described comes from kernels he provided,
and I certainly would still be debugging Gecode if he hadn't stepped me
through dozens of proofs. Thank you, all.

Thanks and great appreciation also go to my partner, Laura Cathey, who has
tolerated my absence far too long. I promise I won't start another project for
at least a few days.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem{borland-honeynet}
  Borland, Matt. Submission to Honeynet.org \emph{Scan of the Month},
  05/05/2001. \url{http://old.honeynet.org/scans/scan15/som/som6.txt}
\bibitem{honeynet}
  The Honeynet Project. \url{http://www.honeynet.org/}
\bibitem{honeynet-15}
  The Honeynet Project \emph{Scan of the Month} \#15.
  \url{http://old.honeynet.org/scans/scan15/}
\bibitem{lee}
  Lee, Json. Submission to Honeynet.org \emph{Scan of the Month}, 05/25/2001.
  \url{http://old.honeynet.org/scans/scan15/som/som33.html}
\bibitem{ocaml}
  Minsky, Yaron and Madhavapeddy, Anil and Hickey, Jason (2013) Chapter 3.
  Lists and Patterns; Options. {\em Real World OCaml}.
  \url{https://realworldocaml.org/v1/en/html/a-guided-tour.html#options}
\bibitem{non-gnu}
  Poirier, Dave. The Second Extended File System.
  \url{http://www.nongnu.org/ext2-doc/ext2.html}
\bibitem{sleuth-kit}
  The Sleuth Kit. \url{http://www.sleuthkit.org/}
\bibitem{scala}
  Wampler, Dean and Payne, Alex (2008) Chapter 8. Functional Programming in
  Scala. {\em Programming Scala}. 
  \url{http://ofps.oreilly.com/titles/9780596155957/FunctionalProgramming.html}

\end{thebibliography}

\appendix

\section{Code Samples*}
Complete code samples and Coq tactics are available on Github:
\url{https://github.com/cmc333333/forensics-thesis-code}

\end{document}
