%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[nocopyrightspace]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,color,listings,lstcoq,url}
\usepackage[T1]{fontenc}

\definecolor{ltblue}{rgb}{0,0.4,0.4}
\definecolor{dkblue}{rgb}{0,0.1,0.6}
\definecolor{dkgreen}{rgb}{0,0.35,0}
\definecolor{dkviolet}{rgb}{0.3,0,0.5}
\definecolor{dkred}{rgb}{0.5,0,0}

\begin{document}

\lstset{language=coq, basicstyle=\ttfamily\scriptsize, columns=flexible,
keepspaces=true}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Formalizing the Honeynet}
\subtitle{Defining and Proving a Rootkit Installation}

\authorinfo{CM Lubinski}
           {DePaul University}
           {cm.lubinski@gmail.com}

\maketitle

\begin{abstract}
Definitions for multiple file formats, "malicious" file names, and forensic
time lines are provided in Coq. These are combined to mimic the types of
evidence given by independent forensics researchers in a Honeynet forensics
competition. The Honeynet examples are then provided in the form of formal
proofs based on these definitions. Along the way, functions for creating
relevant data structures within Coq are also described.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
\terms
term1, term2

\keywords
keyword1, keyword2

\section{Introduction}

After completing a lengthy customs process in the largest airport of
burgeoning global power, you notice that your laptop appears to be running
rather slowly. You begin to suspect that a root-kit has been installed, but
you are uncertain how to prove this. At first, you consider using
off-the-shelf antivirus software to find the infection. The risk of damaging
international relations between your countries is too great when using such
proprietary software, however. What you really need is a concrete definition
of a root kit and evidence fitting that definition to prove that your machine
has been attacked.

This scenario is the ultimate goal of the research provided in this paper. We
need formal (we will use the Coq programming language) definitions for various
types of evidence needed by forensics analysts. We will also need a way to
convert real data (e.g. from a disk image) into these definitions. Combining
these two ideas, we can provide concrete {\em proofs} regarding whether or not
a particular image fits such a definition; in the above example, we could
provide a {\em proof} that a root-kit was installed.

To guide our search, we will focus on the evidence structures and proofs
described by several independent researchers studying a ``Honeynet''
challenge. The Honeynet Project\cite{honeynet}'s now-defunct {\it Scan of the
Month} series provided researchers a disk image attained from a compromised
honeypot (a computer created with the explicit goal of catching malware for
inspection.) Each month, they were challenged to describe what happened to the
system and provide evidence for their conclusions. We will consider one
specific example\cite{honeynet-15}, in which a rootkit was installed on a
server and the security community was asked to recover the rootkit, prove that
it had been installed, and provide a step-by-step writeup describing how the
rootkit was found.

\section{Getting Our Feet Wet with File Types}

We start by considering a relatively straight-forward request: defining what
it means for a given file to be a JPEG. How can we formalize this notion? One
tact (used by many operating systems) is to rely on the file extension (if
present) -- in this case, checking for either {\tt jpg} or {\tt jpeg}. This is
a very loose definition, however, as malicious users would need give their
files a different extension to avoid detection. We could instead review the
JPEG spec and confirm that all of the meta data contained within this file is
consistent with said spec. This approach runs the opposite end of the
spectrum, requiring significantly more evidence. Further, the JPEG spec is not
as tidy as we might hope, and most applications are lenient with the file
formats they accept.

Instead, we will chose a middle route, opting to use ``magic numbers'' as our
guide. This refers to tell-tale byte values at predictable points within the
file data. These file signatures are {\it relatively} unique to various file
formats, so we will use them in our definitions and add additional checks as
necessary. JPEGs happen to always start with the bytes {\tt ff d8} and end
with {\tt ff d9}. Similarly, gzipped files begin with {\tt 1f 8b 08} and Linux
executables (ELFs) begin with {\tt 7f 45 4c 46} ({\tt 7f} E L F).

If we represent each byte as a positive integer, then writing these
definitions in the proof-centric language of Coq (plus some syntactic sugar)
would look like

\begin{lstlisting}
Definition isJpeg (file: File) :=
     file @[  0 ] = value 255
  /\ file @[  1 ] = value 216 
  /\ file @[ -2 ] = value 255
  /\ file @[ -1 ] = value 217.

Definition isGzip (file: File) :=
     file @[ 0 ] = value 31
  /\ file @[ 1 ] = value 139 
  /\ file @[ 2 ] = value 8.

Definition isElf (file: File) :=
     file @[ 0 ] = value 127
  /\ file @[ 1 ] = value 69 
  /\ file @[ 2 ] = value 76
  /\ file @[ 3 ] = value 70.
\end{lstlisting}

For each definition, the first (and, in the case of the JPEG, last) bytes of a
file must both be present (as indicated by {\tt value}) and equal to the byte
sequences described above. We will discuss the absence of byte values later,
but for now, assume that this accounts for ``missing'' evidence. By defining
file types in this manner, we can use these definition as building block
within larger definitions and within proofs. For example, given these
definitions, we can {\it prove} that JPEG files cannot also be gzipped files:

\begin{lstlisting}
Lemma jpeg_is_not_gzip : forall (file: File),
  (isJpeg file) -> ~(isGzip file).
\end{lstlisting}

A full version of this proof (and others mentioned throughout this paper) is
provided in the appendix.

\section{Expanding Definitions with Honeynet Examples}

Let's now consider the Honeynet {\it Scan of the Month} mentioned in the
introduction. In his entry for this contest, Matt 
Borland\cite{borland-honeynet} described a deleted, ``tar/gzipped file
containing the tools necessary for creating a home for the attacker on the
compromised system''. Formalizing this a bit, we will say that he proved that

\begin{lstlisting}
Lemma bordland_honeynet_file:
  exists (file: File),
  (isOnDisk file honeynet_image_a)
  /\ isDeleted file
  /\ isGzip file
  /\ exists (filename: list Z),
     (In filename (Tar.parseFileNames (gunzip_a file)))
     /\ looksLikeRootkit filename.
\end{lstlisting}

That's a bit of a mouthful, but we are stating there exists a deleted, gzipped
file on the Honeynet disk image such that, when we unzip the file, the
contained tar includes a malicious-looking file name. Note that both the
Honeynet image and the unzipping operation are appended with `{\tt \_a}',
which we will use to indicate ``assumption''. We will discuss assumptions in
detail momentarily; we can treat them as another type of definition until
then. We would instead prefer to focus on each of the definitions in the and
clause (save {\tt isGzip}, which has already been described).

\subsection{isOnDisk}

How might we define that a file "exists" on a particular disk image? For a
simple definition, we could start by claiming a ``file'' is on a disk image if
that file's contents could be found sequentially in the disk. In other words,
we might consider the file to be on disk if we can find a starting index on
the disk such that every byte afterwards matches that of the file.

\begin{lstlisting}
Definition isOnDiskTry1 (file: File) (disk: Disk):
  exists (startPosition: Z), forall (i: Z),
  (i >= 0 /\ i < file.(fileSize)) -> 
    disk @[ startPosition + i ] = file @[ i ].
\end{lstlisting}

This definition isn't very useful in practice, however, as files are very
often fragmented across multiple segments of a disk image. Moreover, this
definition would also include false positives, where a ``file'' is formed by
looking at the bits that span a fragment boundary. Files are stored on disks
via file systems, which make no sequential guarantees (particularly in recent
file systems such as ZFS and Btrfs).

We must therefore, build our definition for file existence with file systems
in mind. Proving that a file exists within each type of file system is a
relatively unique operation, however. As a result, we will define file
existence as the disjunction of file existence within each file system. While
the definitions are distinct, they each tend to follow the pattern that there
exists some file identifier such that if we were to parse the file associated
with that identifier on the given disk, we would find a file identical to the
provided file.

\begin{lstlisting}
Definition isOnDisk (file: File) (disk: Disk):
  (* Ext2 *)
  (exists (inodeIndex: Z):
    (parseFileFromInodeIndex disk inodeIndex) = value file)
  \/ (* FAT32 *)
  (exists (clusterNumber: Z):
    (parseFileFromClusterNumber disk clusterNumber) = value file)
  \/ (* Btrfs *)
  (exists (key: Z):
    (parseFileFromBtrfsKey disk key) = value file)
  \/ ...
\end{lstlisting}

The functions {\tt parseFileFrom}* perform the work of inspecting the disk
image, reading the relevant data structures, and creating an abstract
representation of the results (in this case, a File object.) We will discuss
parsing computations a bit later.

\subsection{isDeleted}

Like existence on the disk, what it means for a file to be deleted is quite
file system specific. In Ext2, a file might be seen as deleted if its bit in
the allocation bitmap were zero; in FAT32, we might consider a file deleted if
it were not accessible via a cluster chain. To account for these varieties, we
include will include the deletion status of a File within its definition. This
definition attempts to describe ``universal'' file truths:

\begin{lstlisting}
Structure File := mkFile {
  fileSize: Z;
  deleted: boolean;
  byteOffset: Z->Exc Z
}.
\end{lstlisting}

The first two fields are self-explanatory, but the third warrants additional
explanation. This field is a function that, when given an offset within a
file, returns the byte value found at that offset within the file. As
described in our investigation of file types, we must account for the
possibility that the byte requested is not available (e.g. out of range or not
within our assumptions). We must wrap the result in an Exc (a.k.a. Option) to
handle this situation.

Coming back to goal, it should be easy to see that the definition of {\tt
isDeleted} effectively just delegates to the boolean value that is part of the
File representation:

\begin{lstlisting}
Definition isDeleted (file: File):
  file.(deleted) = true.
\end{lstlisting}

\subsection{Malicious Filenames}

The final clause in our lemma states that one of the filenames contained in
the gzipped-tar is suspicious. We start with  another assumption, in this case
the decompression algorithm for gzipped files; while we try to keep as many of
our computations within Coq as possible, some (like unzipping a file) were far
out of scope, so we keep them as assumptions. Once the file is unzipped, we
run another parsing computation, {\tt Tar.parseFileNames}, this time
retrieving all of the file names contained within the unzipped tar.

The only new {\it definition} is that of {\tt looksLikeRootkit}. For the sake
of this definition, we will define that a filename looks like a filename
involved in a rootkit if that filename, excluding its path prefix, matches any
of a predefined list of system files. These include task managers such as {\tt
top} and {\tt ProcMon.exe}, which root kits replace to hide their activities,
as well as {\tt ssh} and {\tt rsync}, which root kits replace to allow them
the ability to monitor traffic. We treat file names as lists of bytes to
account for unicode and other non-ascii characters which would be excluded by
Coq's string/ascii package.

\begin{lstlisting}
Definition looksLikeRootkit (fileName: list Z):
  In (trimFileNamePrefix fileName) 
     (map ascii2Bytes ("ps" :: "netstat" :: "top" :: "ifconfig" 
                       :: "ssh" :: "rsync" :: "ProcMon.exe" 
                       :: nil)).
\end{lstlisting}

Note that, for ease of description, we provide file names as ascii strings and
then convert each to their equivalent, byte representation ({\tt list Z}) via
the {\tt ascii2Bytes} function.

\section{Lemmas, Computations, and Definitions; Oh My!}

We have now come across several core concepts of this research, and it is easy
to see how they might be confused. Before we continue, let us solidify our
understanding of each category.

We are most familiar with {\bf definitions}, which provide a name for a common
understanding of a concept. For example, we provided definitions for {\tt
isGzip}, {\tt looksLikeRootkit}, etc. The wider goal of this research is to
provide a set of common definitions for concepts pertinent to the forensics
community. This paper describes several which are relevant to the Honeynet
example, but other definitions might include ``web page access'' (e.g. by
inspecting browser history), ``in contact with'' (e.g. there exists records of
email communication), and ``last time logged in''.

Definitions are most often built by combining aspects of various abstract {\bf
data structures}, ``universal'' representations of data-related concepts
within forensics. We have seen these used to represent files, and we will see
them represent various Ext2 file system structures, as well as events in a
reconstructed timeline. These structures serve as a way of differentiating
particular parsing computations (which build the structures) from definitions
involving these structures. It should be relatively simple for someone to
write a different parsing algorithm which produces the same structures without
affecting any of the definitions.

Unfortunately, the Coq proof environment is quite limited with regards to
practical applications. It cannot, for example, read bytes from a disk image;
it cannot (for our intents) call external programs to transform data; it
cannot even instantiate a list of tens of thousands of values. We work around
these limitations by making {\bf assumptions} about our environment (with the
suffix {\tt \_a}) which entail only the relevant pieces of data needed for our
proofs. We won't read a file from disk; instead, we will generate a {\it
sparse} map to represent that disk, including only the offsets we care about.
Assumptions are also used to account for transformations that would normally
be performed by a trusted, external program. We cannot, for instance, run
``gunzip'', nor do we want to try to implement that algorithm within Coq.
Instead, we require an assumption be made that encompasses that activity. Our
proofs cannot validate that {\tt disk\_a} or {\tt gunzip\_a} accurately
represent values on the disk or the output of an unzipping operation -- they
instead trust those assumptions to be accurate and rely on users for
verification.

Where possible, we do attempt to {\bf parse} and/or {\bf compute} values
within Coq. Here we have codified algorithms which take the bytes from disk
(assumptions) and convert them into relevant structures. This might mean
creating Inodes, SuperBlocks, and GroupDescriptors for a disk with Ext2 on it,
or reading headers from the data associated with an abstract tar file. In all
cases, these operations are our (perhaps flawed) implementations of various
specifications; users should feel free to check the structures created against
those found using other tools. Ultimately, we aimed to provide definitions and
data structures which could serve as building blocks for proofs; parsing and
computations are simply the glue that tie those building blocks to actual
data.

The Honeynet competition we've described required researchers to provide
``{\bf evidence}'' or ``proof'' that the disk had been compromised by a
rootkit. This required that the researchers both define what acceptable
evidence would look like as well as provide it the evidence regarding the
attacked server. This led each researcher to provide different types of
evidence depending on what that researcher found to be acceptable. While we
could create definitions for the various types of evidence, we will instead
follow the Honeynet applicants' lead and provide only the evidence itself in
the form of a {\bf lemma} or proof. This evidence will be in the form of a
proof term, which the author of the proof can build up using Coq's tactic
system. It is then up to the reviewer to determine whether or not the evidence
provided by an applicant fits a familiar definition.

Finally, the code written for this research is sprinkled with {\bf utility
functions} to perform (mostly) simple transformations. {\tt
trimFileNamePrefix} and {\tt ascii2Bytes} are two such examples which make
explaining (and hopefully, understanding) the code samples easier. These
functions are not necessary for the definitions, proofs, etc. where they
appear; they simply make the code more terse.

\section{Computing Our First Lemma}

\section{Timelines as Evidence}

\section{Nonesense Past This Point}

\section{The Assumption Type}

Coq, as a theorem proving language, has limited applicability for certain
practical applications. Due to its constructive representation of natural
numbers ({\tt nat}), for example, attempting to use this type to represent
"large" numbers as small at 8000 may cause the interpreter to overflow its
heap. This means that we make use of binary-representations for numbers and
use sparse maps to represent disk images. 

Their focus also proofs also leads to a frustrating lack of substantial,
practical library support. Further, there is no foreign function interface
which would allow 


and evidence for this particular competition would complete the proof for

\begin{lstlisting}
Lemma borland_15_proof: 
  (bordland_malicious_tgz honeynet_15_image 23).
\end{lstlisting}

It is important to point out that in Borland's writeup (and in many forensics
papers) the same person is both authoring the {\it definition} of evidence as
well as {\it providing the evidence} itself. This is exactly the problem
discussed above, the sort of biased results we are trying to avoid.

Let's dig into our definition a bit deeper. We start by declaring that there
{\it exists} a file such that the file matches that received from reading
inodes on the disk. This is particular to 

%Note that, while we were given an entire file, we only cared about the initial
%and final bytes. As our definition does not involve any other bytes of the
%image, these bytes could be all zeros, all ones, present, damaged, or
%otherwise. The definition doesn't care, and this is a trait we will use in the
%future to limit the size of our proofs.

\section{Delete INode}

Now, let's consider a more complicated example, that of testing whether a
particular inode index is marked as ``deleted'' in the file system. Note that
there are many possible definitions of ``deleted'' we may choose from. The
ext2 file system has redundant mechanisms to define if a file has been
deleted. We chose one, the allocation bit map. Ultimately, whether or not a
given inode index is marked as deleted involves finding the ``group block''
associated with that index, looking up the bit associated with the index in
the allocation table, and checking whether it is zero (for deleted) or one
(for allocated).

Of course, there is a lot we glossed over. What is a group block? How do we
find the right one? How do we know that the inode index provided is valid? To
answer these questions, we created several additional data structures and
evidence functions.

\section{Future Work}

automatically generate the proof based on disk images; additional types of
evidence; abstractions of file systems


\appendix
\section{Relevant Proofs}

\begin{lstlisting}
Lemma jpeg_is_not_tgz : forall (file: File),
  (isJpeg file) -> ~(isTgz file).
  Proof.
  unfold isTgz, isJpeg.
  intros file jpeg_asmpt.
  destruct jpeg_asmpt as [byte0_is_255].
  rewrite byte0_is_255.
  unfold not. intros contra.
  destruct contra as [not_equal].
  discriminate not_equal.
  Qed.
\end{lstlisting}

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem{honeynet}
The Honeynet Project. \url{http://www.honeynet.org/}
\bibitem{honeynet-15}
The Honeynet Project \emph{Scan of the Month} \#15.
\url{http://old.honeynet.org/scans/scan15/}
\bibitem{borland-honeynet}
Borland, Matt. Submission to Honeynet.org \emph{Scan of the Month},
05/05/2001. \url{http://old.honeynet.org/scans/scan15/som/som6.txt}

\end{thebibliography}


\end{document}
