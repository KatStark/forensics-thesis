%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[nocopyrightspace]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,color,listings,lstcoq,url}
\usepackage[T1]{fontenc}

\definecolor{ltblue}{rgb}{0,0.4,0.4}
\definecolor{dkblue}{rgb}{0,0.1,0.6}
\definecolor{dkgreen}{rgb}{0,0.35,0}
\definecolor{dkviolet}{rgb}{0.3,0,0.5}
\definecolor{dkred}{rgb}{0.5,0,0}

\begin{document}

\lstset{language=coq, basicstyle=\ttfamily\scriptsize, columns=flexible,
keepspaces=true}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Formalizing the Honeynet}
\subtitle{Defining and Proving a Rootkit Installation}

\authorinfo{CM Lubinski}
           {DePaul University}
           {cm.lubinski@gmail.com}

\maketitle

\begin{abstract}
Definitions for multiple file formats, "malicious" file names, and forensic
time lines are provided in Coq. These are combined to mimic the types of
evidence given by independent forensics researchers in a Honeynet forensics
competition. The Honeynet examples are then provided in the form of formal
proofs based on these definitions. Along the way, functions for creating
relevant data structures within Coq are also described.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
\terms
term1, term2

\keywords
keyword1, keyword2

\section{Introduction}

After completing a lengthy customs process in the largest airport of
burgeoning global power, you notice that your laptop appears to be running
rather slowly. You begin to suspect that a root-kit has been installed, but
you are uncertain how to prove this. At first, you consider using
off-the-shelf antivirus software to find the infection. The risk of damaging
international relations between your countries is too great when using such
proprietary software, however. What you really need is a concrete definition
of a root kit and evidence fitting that definition to prove that your machine
has been attacked.

This scenario is the ultimate goal of the research provided in this paper. We
need formal (we will use the Coq programming language) definitions for various
types of evidence needed by forensics analysts. We will also need a way to
convert real data (e.g. from a disk image) into these definitions. Combining
these two ideas, we can provide concrete {\em proofs} regarding whether or not
a particular image fits such a definition; in the above example, we could
provide a {\em proof} that a root-kit was installed.

To guide our search, we will focus on the evidence structures and proofs
described by several independent researchers studying a ``Honeynet''
challenge. The Honeynet Project\cite{honeynet}'s now-defunct {\it Scan of the
Month} series provided researchers a disk image attained from a compromised
honeypot (a computer created with the explicit goal of catching malware for
inspection.) Each month, they were challenged to describe what happened to the
system and provide evidence for their conclusions. We will consider one
specific example\cite{honeynet-15}, in which a rootkit was installed on a
server and the security community was asked to recover the rootkit, prove that
it had been installed, and provide a step-by-step writeup describing how the
rootkit was found.

\section{Getting Our Feet Wet with File Types}

We start by considering a relatively straight-forward request: defining what
it means for a given file to be a JPEG. How can we formalize this notion? One
tact (used by many operating systems) is to rely on the file extension (if
present) -- in this case, checking for either {\tt jpg} or {\tt jpeg}. This is
a very loose definition, however, as malicious users would need give their
files a different extension to avoid detection. We could instead review the
JPEG spec and confirm that all of the meta data contained within this file is
consistent with said spec. This approach runs the opposite end of the
spectrum, requiring significantly more evidence. Further, the JPEG spec is not
as tidy as we might hope, and most applications are lenient with the file
formats they accept.

Instead, we will chose a middle route, opting to use ``magic numbers'' as our
guide. This refers to tell-tale byte values at predictable points within the
file data. These file signatures are {\it relatively} unique to various file
formats, so we will use them in our definitions and add additional checks as
necessary. JPEGs happen to always start with the bytes {\tt ff d8} and end
with {\tt ff d9}. Similarly, gzipped files begin with {\tt 1f 8b 08} and Linux
executables (ELFs) begin with {\tt 7f 45 4c 46} ({\tt 7f} E L F).

If we represent each byte as a positive integer, then writing these
definitions in the proof-centric language of Coq (plus some syntactic sugar)
would look like

\begin{lstlisting}
Definition isJpeg (file: File) :=
     file @[  0 ] = value 255
  /\ file @[  1 ] = value 216 
  /\ file @[ -2 ] = value 255
  /\ file @[ -1 ] = value 217.

Definition isGzip (file: File) :=
     file @[ 0 ] = value 31
  /\ file @[ 1 ] = value 139 
  /\ file @[ 2 ] = value 8.

Definition isElf (file: File) :=
     file @[ 0 ] = value 127
  /\ file @[ 1 ] = value 69 
  /\ file @[ 2 ] = value 76
  /\ file @[ 3 ] = value 70.
\end{lstlisting}

For each definition, the first (and, in the case of the JPEG, last) bytes of a
file must both be present (as indicated by {\tt value}) and equal to the byte
sequences described above. We will discuss the absence of byte values later,
but for now, assume that this accounts for ``missing'' evidence. By defining
file types in this manner, we can use these definition as building block
within larger definitions and within proofs. For example, given these
definitions, we can {\it prove} that JPEG files cannot also be gzipped files:

\begin{lstlisting}
Lemma jpeg_is_not_gzip : forall (file: File),
  (isJpeg file) -> ~(isGzip file).
\end{lstlisting}

A full version of this proof (and others mentioned throughout this paper) is
provided in the appendix.

\section{Expanding Definitions with Honeynet Examples}

Let's now consider the Honeynet {\it Scan of the Month} mentioned in the
introduction. In his entry for this contest, Matt 
Borland\cite{borland-honeynet} described a deleted, ``tar/gzipped file
containing the tools necessary for creating a home for the attacker on the
compromised system''. Formalizing this a bit, we will say that he proved that

\begin{lstlisting}
Lemma bordland_honeynet_file:
  exists (file: File),
  (isOnDisk file honeynet_image_a)
  /\ isDeleted file
  /\ isGzip file
  /\ exists (filename: ByteString),
     (In filename (Tar.parseFileNames (gunzip_a file)))
     /\ looksLikeRootkit filename.
\end{lstlisting}

That's a bit of a mouthful, but we are stating there exists a deleted, gzipped
file on the Honeynet disk image such that, when we unzip the file, the
contained tar includes a malicious-looking file name. Note that both the
Honeynet image and the unzipping operation are appended with `{\tt \_a}',
which we will use to indicate ``assumption''. We will discuss assumptions in
detail momentarily; we can treat them as another type of definition until
then. We would instead prefer to focus on each of the definitions in the and
clause (save {\tt isGzip}, which has already been described).

\subsection{isOnDisk}

How might we define that a file "exists" on a particular disk image? For a
simple definition, we could start by claiming a ``file'' is on a disk image if
that file's contents could be found sequentially in the disk. In other words,
we might consider the file to be on disk if we can find a starting index on
the disk such that every byte afterwards matches that of the file.

\begin{lstlisting}
Definition isOnDiskTry1 (file: File) (disk: Disk):
  exists (startPosition: Z), forall (i: Z),
  (i >= 0 /\ i < file.(fileSize)) -> 
    disk @[ startPosition + i ] = file @[ i ].
\end{lstlisting}

This definition isn't very useful in practice, however, as files are very
often fragmented across multiple segments of a disk image. Moreover, this
definition would also include false positives, where a ``file'' is formed by
looking at the bits that span a fragment boundary. Files are stored on disks
via file systems, which make no sequential guarantees (particularly in recent
file systems such as ZFS and Btrfs).

We must therefore, build our definition for file existence with file systems
in mind. Proving that a file exists within each type of file system is a
relatively unique operation, however. As a result, we will define file
existence as the disjunction of file existence within each file system. While
the definitions are distinct, they each tend to follow the pattern that there
exists some file identifier such that if we were to parse the file associated
with that identifier on the given disk, we would find a file identical to the
provided file.

\begin{lstlisting}
Definition isOnDisk (file: File) (disk: Disk):
  (* Ext2 *)
  (exists (inodeIndex: Z):
    (parseFileFromInodeIndex disk inodeIndex) = value file)
  \/ (* FAT32 *)
  (exists (clusterNumber: Z):
    (parseFileFromClusterNumber disk clusterNumber) = value file)
  \/ (* Btrfs *)
  (exists (key: Z):
    (parseFileFromBtrfsKey disk key) = value file)
  \/ ...
\end{lstlisting}

The functions {\tt parseFileFrom}* perform the work of inspecting the disk
image, reading the relevant data structures, and creating an abstract
representation of the results (in this case, a File object.) We will discuss
parsing computations a bit later.

\subsection{isDeleted}

Like existence on the disk, what it means for a file to be deleted is quite
file system specific. In Ext2, a file might be seen as deleted if its bit in
the allocation bitmap were zero; in FAT32, we might consider a file deleted if
it were not accessible via a cluster chain. To account for these varieties, we
include will include the deletion status of a File within its definition. This
definition attempts to describe ``universal'' file truths:

\begin{lstlisting}
Structure File := mkFile {
  fileSize: Z;
  deleted: boolean;
  byteOffset: Z->Exc Z
}.
\end{lstlisting}

The first two fields are self-explanatory, but the third warrants additional
explanation. This field is a function that, when given an offset within a
file, returns the byte value found at that offset within the file. As
described in our investigation of file types, we must account for the
possibility that the byte requested is not available (e.g. out of range or not
within our assumptions). We must wrap the result in an Exc (a.k.a. Option) to
handle this situation.

Coming back to goal, it should be easy to see that the definition of {\tt
isDeleted} effectively just delegates to the boolean value that is part of the
File representation:

\begin{lstlisting}
Definition isDeleted (file: File):
  file.(deleted) = true.
\end{lstlisting}

\subsection{Malicious Filenames}

The final clause in our lemma states that one of the filenames contained in
the gzipped-tar is suspicious. We start with  another assumption, in this case
the decompression algorithm for gzipped files; while we try to keep as many of
our computations within Coq as possible, some (like unzipping a file) were far
out of scope, so we keep them as assumptions. Once the file is unzipped, we
run another parsing computation, {\tt Tar.parseFileNames}, this time
retrieving all of the file names contained within the unzipped tar.

The only new {\it definition} is that of {\tt looksLikeRootkit}. For the sake
of this definition, we will define that a filename looks like a filename
involved in a rootkit if that filename, excluding its path prefix, matches any
of a predefined list of system files. These include task managers such as {\tt
top} and {\tt ProcMon.exe}, which root kits replace to hide their activities,
as well as {\tt ssh} and {\tt rsync}, which root kits replace to allow them
the ability to monitor traffic. We treat file names as lists of bytes to
account for unicode and other non-ascii characters which would be excluded by
Coq's string/ascii package.

\begin{lstlisting}
Definition looksLikeRootkit (fileName: ByteString):
  In (trimFileNamePrefix fileName) 
     (map ascii2Bytes ("ps" :: "netstat" :: "top" :: "ifconfig" 
                       :: "ssh" :: "rsync" :: "ProcMon.exe" 
                       :: nil)).
\end{lstlisting}

Note that, for ease of description, we provide file names as ascii strings and
then convert each to their equivalent, byte representation via the {\tt
ascii2Bytes} function.

\section{Lemmas, Computations, and Definitions; Oh My!}

We have now come across several core concepts of this research, and it is easy
to see how they might be confused. Before we continue, let us solidify our
understanding of each category.

We are most familiar with {\bf definitions}, which provide a name for a common
understanding of a concept. For example, we provided definitions for {\tt
isGzip}, {\tt looksLikeRootkit}, etc. The wider goal of this research is to
provide a set of common definitions for concepts pertinent to the forensics
community. This paper describes several which are relevant to the Honeynet
example, but other definitions might include ``web page access'' (e.g. by
inspecting browser history), ``in contact with'' (e.g. there exists records of
email communication), and ``last time logged in''.

Definitions are most often built by combining aspects of various abstract {\bf
data structures}, ``universal'' representations of data-related concepts
within forensics. We have seen these used to represent files, and we will see
them represent various Ext2 file system structures, as well as events in a
reconstructed timeline. These structures serve as a way of differentiating
particular parsing computations (which build the structures) from definitions
involving these structures. It should be relatively simple for someone to
write a different parsing algorithm which produces the same structures without
affecting any of the definitions.

Unfortunately, the Coq proof environment is quite limited with regards to
practical applications. It cannot, for example, read bytes from a disk image;
it cannot (for our intents) call external programs to transform data; it
cannot even instantiate a list of tens of thousands of values. We work around
these limitations by making {\bf assumptions} about our environment (with the
suffix {\tt \_a}) which entail only the relevant pieces of data needed for our
proofs. We won't read a file from disk; instead, we will generate a {\it
sparse} map to represent that disk, including only the offsets we care about.
Assumptions are also used to account for transformations that would normally
be performed by a trusted, external program. We cannot, for instance, run
``gunzip'', nor do we want to try to implement that algorithm within Coq.
Instead, we require an assumption be made that encompasses that activity. Our
proofs cannot validate that {\tt disk\_a} or {\tt gunzip\_a} accurately
represent values on the disk or the output of an unzipping operation -- they
instead trust those assumptions to be accurate and rely on users for
verification.

Where possible, we do attempt to {\bf parse} and/or {\bf compute} values
within Coq. Here we have codified algorithms which take the bytes from disk
(assumptions) and convert them into relevant structures. This might mean
creating Inodes, SuperBlocks, and GroupDescriptors for a disk with Ext2 on it,
or reading headers from the data associated with an abstract tar file. In all
cases, these operations are our (perhaps flawed) implementations of various
specifications; users should feel free to check the structures created against
those found using other tools. Ultimately, we aimed to provide definitions and
data structures which could serve as building blocks for proofs; parsing and
computations are simply the glue that tie those building blocks to actual
data.

The Honeynet competition we've described required researchers to provide
``{\bf evidence}'' or ``proof'' that the disk had been compromised by a
rootkit. This required that the researchers both define what acceptable
evidence would look like as well as provide it the evidence regarding the
attacked server. This led each researcher to provide different types of
evidence depending on what that researcher found to be acceptable. While we
could create definitions for the various types of evidence, we will instead
follow the Honeynet applicants' lead and provide only the evidence itself in
the form of a {\bf lemma} or proof. This evidence will be in the form of a
proof term, which the author of the proof can build up using Coq's tactic
system. It is then up to the reviewer to determine whether or not the evidence
provided by an applicant fits a familiar definition.

Finally, the code written for this research is sprinkled with {\bf utility
functions} to perform (mostly) simple transformations. {\tt
trimFileNamePrefix} and {\tt ascii2Bytes} are two such examples which make
explaining (and hopefully, understanding) the code samples easier. These
functions are not necessary for the definitions, proofs, etc. where they
appear; they simply make the code more terse.

\section{Computing Our First Lemma}

Now that we have introduced each of these contents, let's step through the
proof for our {\tt bordland\_honeynet\_file} proof, which will touch on each.

\subsection{Assumptions}
Our proof will make use of two assumptions: {\tt honeynet\_image\_a}, a {\tt
Disk} and {\tt gunzip\_a}, a function, {\tt File->File}. The {\tt Disk} type
is used to represent a disk image throughout our definitions and proofs.
Technically, they are functions from {\tt Z} to {\tt Exc Z}, acting as
byte-retrieval mechanism; give a {\tt Disk} an offset, and it will respond
with the byte value (a {\tt Z}) at that offset in the disk image, or {\tt
error} if no such byte exists on the disk. Generally, disk data is backed by
an in-memory mapping via Coq's {\tt FMapAVL} trees, and must be generated from
the true disk values. Due to Coq's limitations on data structure sizes, we
only populate these maps with essential values; once the map reaches a few
thousand entries, it will no longer be usable within Coq.

The second assumption represents gzip's unzipping/deflating operation. As
implementing the decompression algorithm used within Coq would not provide
particularly valuable information, we instead delegate its operation to an
assumption. This assumption needs to be able to convert a compressed, gzip
file into the corresponding, uncompressed file, which effectively means
providing a new {\tt File} populated with the relevant bytes. In our example,
we used the {\it Sleuth Kit}'s {\tt icat} program to pull inode {\tt 23}'s
contents from the disk image, ran {\tt gunzip} on the resulting {\tt tgz}
file, and then pulled the relevant bytes (i.e. those necessary from the tar
file headers). We effectively just wrap this data in a function that,
regardless of {\tt File} input, returns the uncompressed, tar {\tt File}.

\subsection{Parsing a File via INode}

Our evidence relies on the {\it existence} of a {\tt File} which is on disk,
deleted, etc., so our proof for this evidence need only provide such a {\tt
File}, and it will be easiest to pull that file from the disk image. We
therefore use a parsing function (rather, series of functions) to get this.
Ultimately, we will simply call and provide as demonstrative proof

\begin{lstlisting}
(Inode.parseFileFromInodeIndex honeynet_image_a 23)
\end{lstlisting}

To get to that point where we are confident in the results of this function,
we must peal off each layer and review each constructed data structure.

\subsubsection{parseFileFromInodeIndex}

At the outermost conceptual layer, we have a function which, when given a disk
and Ext2 Inode identifier, returns either an error or a valid File structure
associated with that file. As we discussed earlier, a {\tt File} is composed
of a deletion status, a file size, and a function that retrieves bytes within
the file. Retrieving each of these fields relies on first parsing out the
Superblock (the ``root'' structure in the Ext2 file system) and the Group
Descriptor associated with the inode; the file size and function which
retrieves bytes further rely on parsing out the inode structure. Each of those
parsing computations can fail, and if this occurs, we want the entire {\tt
File} parsing function to propagate the failure. A functional solution to the
problem is to use a monadic approach, taking each step in the computation only
if previous result was successful, and ``flattening'' the results. 

\begin{lstlisting}
Function parseFileFromInodeIndex (disk: Disk) (inodeIndex: Z) 
: Exc File :=
  (Ext2.findAndParseSuperBlock disk) flatmap (fun superblock =>
    let groupId := 
      inodeIndex / superblock.(numInodesInBlockGroup) in
    let inodeIndexInGroup := 
      inodeIndex mod superblock.(numInodesInBlockGroup) in
    (Ext2.findAndParseGroupDescriptor disk superblock 
                               groupId) flatmap (
      fun groupDesc =>

      (Ext2.parseInode disk superblock groupDesc 
                       inodeIndex) map (
        fun inode =>

        mkFile
          inode.(lowerBitsFileSize)
          (Ext2.parseDeleted disk superblock groupDesc
                             inodeIndexInGroup)
          (Inode.fetchFileByte disk inode)
      )
    )
  )
.
\end{lstlisting}

As a quick primer on Ext2, each file has an associated Inode structure on
disk, which contains information about the file's size, modification times,
etc., and is identified by a positive, integer index. Inodes are grouped into
collections such that each collection (a ``Block Group'') can aggregate data
about the Inodes, such as which inodes are allocated, how many are
directories, etc. in a ``Group Descriptor'' structure. Meta information about
the whole file system, such as the number of Inodes in each group, is stored
in the ``SuperBlock''.

\subsubsection{findAndParseSuperBlock}

The SuperBlock structure is generally located at multiple positions on a disk
image for redundancy. The first is always positioned after any boot code on
the disk, starting at the 1024th byte. While the size of the SuperBlock
depends on the revision of Ext, the spec is largely compatible, so we will use
the SuperBlock structure described by David Poirier\cite{non-gnu}, which cares
only about the first 264 or so bytes of the 1024 allocation.

Finding and parsing the SuperBlock, then, amounts to jumping to offset 1024 on
the disk and plugging in the read values into a SuperBlock structure. We will
use a {\tt shift} function to effectively reset the ``first'' offset in the
disk @todo. The values to be stored are largely encoded as little endian byte
sequences, so we will make frequent use of an unsigned conversion function,
{\tt seq\_lendu}. While we omit most of the fields, the function looks
something like:

\begin{lstlisting}
Function findAndParseSuperBlock (disk: Disk): Exc SuperBlock :=
  let disk := (shift disk 1024) in
  (seq_lendu disk 0 4) flatmap (fun inodesCount =>
    (seq_lendu disk 4 4) flatmap (fun blocksCount =>
      (seq_lendu disk 8 4) flatmap (fun reservedBlocksCount =>
        (* ... Additional fields omitted ... *)

        (seq_lendu disk 260 4) map (fun firstMetaBg =>
          mkSuperBlock
            inodesCount
            blocksCount
            reservedBlocksCount
            (* ... Additional fields omitted ... *)
            firstMetaBg
        )
        (* ... Additional parentheses omitted ... *)
      )
    )
  ).
\end{lstlisting}

While we parse out virtually all of the fields of a SuperBlock, we will need
only a handful. {\tt inodesCount} provides an upper bound for inode indices
(which we use to validate that a given inode exists). {\tt inodesPerGroup}
will appear several times in this paper; the field indicates the number of
inodes assigned to each GroupDescriptor. Hence, that field provides a way to
determine which GroupDescriptor is needed for a particular inode index. The
{\tt logBlockSize} field is also of interest, as we will use it to determine
the number of bytes each block on disk spans. This is encoded using the
logarithmic scale, but we provide a simple function to convert to the true
number of bytes.

\begin{lstlisting}
Function blockSize (superblock: SuperBlock) := 
  Z.shiftl 1024 superblock.(logBlockSize).
\end{lstlisting}

\subsubsection{Block Addresses}

Below the level of SuperBlocks, the Ext2 file system breaks the disk into
``blocks'' for reference purposes. Despite our use cases, in practice, rarely
does the need to retrieve a single byte from the disk arise on its own.
Instead, data is most often retrieved in sequential chunks, so to minimize
bandwidth, references to specific parts of the disk are identified by ``block
address''. These are akin to cylinders in the FAT32 file system, and the
conversion between a block address and a byte position on the disk is
relatively straight forward. Given the size of a block, treat the disk as an
array of blocks and simply jump to the relevant position.

\begin{lstlisting}
Function ba2Offset (superblock: SuperBlock) (blockAddress: Z) :=
  1024 (* Boot sector *)
  (* 1-indexed *)
  + ((blockSize superblock) * (blockAddress + 1))
.
\end{lstlisting}

\subsubsection{findAndParseGroupDescriptor}

GroupDescriptors will be represented with a similar structure, though with far
fewer fields. GroupDescriptors have 32 allocated bytes, though only 20 are
used, and an array of all GroupDescriptors can be found in the block
immediately following a SuperBlock. Finding a specific GroupDescriptor
therefore requires that we first use the superblock to determine where this
array begins, and we jump to the offset of the group that we care about within
that array.

\begin{lstlisting}
Function findAndParseGroupDescriptor 
  (disk: Disk) (superblock: SuperBlock) (groupId: Z)
  : Exc GroupDescriptor :=
  let groupBlockArrayOffset := 1024 + (blockSize superblock) in
  let descriptorOffset := 32 * groupId in
  let disk := (shift disk (groupBlockArrayOffset 
                           + descriptorOffset)) in
  (seq_lendu disk 0 4) flatmap (fun blockBitmap =>
    (seq_lendu disk 4 4) flatmap (fun inodeBitmap =>
      (seq_lendu disk 8 4) flatmap (fun inodeTable =>
        (seq_lendu disk 12 2) flatmap (fun freeBlocksCount =>
          (seq_lendu disk 14 2) flatmap (fun freeInodesCount =>
            (seq_lendu disk 16 2) flatmap (fun usedDirsCount =>
              (seq_lendu disk 18 2) map (fun pad =>
                mkGroupDescriptor
                  blockBitmap
                  inodeBitmap
                  inodeTable
                  freeBlocksCount
                  freeInodesCount
                  usedDirsCount
                  pad
              )
            )
          )
        )
      )
    )
  )
.
\end{lstlisting}

Two of the fields from this structure will be particularly useful, the {\tt
inodeBitmap} and the {\tt inodeTable}. Both contain block addresses, pointing
to the start of a sequence of inode-related data. The former is simply an
array where each bit represents whether or not the corresponding Inode is
allocated; this will be of great use when we are calculating deletion status.
The latter points to an array of Inode structures, as will be described
shortly.

\subsubsection{parseInode}

\subsubsection{parseDeleted}

\subsubsection{fetchFileBytes}

\begin{lstlisting}
      let bitmapStart := (ba2Offset 
                            superblock
                            groupDesc.(startBlockAddrOfInodeBitmap)) in
      (* Fetch the allocation byte for this inode *)
      (disk (bitmapStart + (inodeIndexInGroup/8))) map (fun allocationByte =>
        (andb
          (* Valid INode *)
          (inodeIndex <=? superblock.(numInodes))

          (* The bit associated with this inode is 0 *)
          (negb (Z.testbit allocationByte (inodeIndexInGroup mod 8)))
        )
      )
    (Ext2.parseInodeFrom disk inodeIndex) map (fun inode =>
      mkFile
        inode.(lowerBitsFileSize)
        deleted
        (Inode.fetchFileByte disk)
    )
  )
.
\end{lstlisting}

\section{Timelines as Evidence}

\section{Nonesense Past This Point}

It is important to point out that in Borland's writeup (and in many forensics
papers) the same person is both authoring the {\it definition} of evidence as
well as {\it providing the evidence} itself. This is exactly the problem
discussed above, the sort of biased results we are trying to avoid.

Let's dig into our definition a bit deeper. We start by declaring that there
{\it exists} a file such that the file matches that received from reading
inodes on the disk. This is particular to 

%Note that, while we were given an entire file, we only cared about the initial
%and final bytes. As our definition does not involve any other bytes of the
%image, these bytes could be all zeros, all ones, present, damaged, or
%otherwise. The definition doesn't care, and this is a trait we will use in the
%future to limit the size of our proofs.

\section{Delete INode}

Now, let's consider a more complicated example, that of testing whether a
particular inode index is marked as ``deleted'' in the file system. Note that
there are many possible definitions of ``deleted'' we may choose from. The
ext2 file system has redundant mechanisms to define if a file has been
deleted. We chose one, the allocation bit map. Ultimately, whether or not a
given inode index is marked as deleted involves finding the ``group block''
associated with that index, looking up the bit associated with the index in
the allocation table, and checking whether it is zero (for deleted) or one
(for allocated).

Of course, there is a lot we glossed over. What is a group block? How do we
find the right one? How do we know that the inode index provided is valid? To
answer these questions, we created several additional data structures and
evidence functions.

\section{Future Work}

automatically generate the proof based on disk images; additional types of
evidence; abstractions of file systems


\appendix
\section{Relevant Proofs}

\begin{lstlisting}
Lemma jpeg_is_not_tgz : forall (file: File),
  (isJpeg file) -> ~(isTgz file).
  Proof.
  unfold isTgz, isJpeg.
  intros file jpeg_asmpt.
  destruct jpeg_asmpt as [byte0_is_255].
  rewrite byte0_is_255.
  unfold not. intros contra.
  destruct contra as [not_equal].
  discriminate not_equal.
  Qed.
\end{lstlisting}

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem{honeynet}
  The Honeynet Project. \url{http://www.honeynet.org/}
\bibitem{honeynet-15}
  The Honeynet Project \emph{Scan of the Month} \#15.
  \url{http://old.honeynet.org/scans/scan15/}
\bibitem{borland-honeynet}
  Borland, Matt. Submission to Honeynet.org \emph{Scan of the Month},
  05/05/2001. \url{http://old.honeynet.org/scans/scan15/som/som6.txt}
\bibitem{sleuth-kit}
  Sleuth Kit
\bibitem{non-gnu}
  http://www.nongnu.org/ext2-doc/ext2.html

\end{thebibliography}


\end{document}
