%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[nocopyrightspace]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,color,listings,lstcoq,url}
\usepackage[T1]{fontenc}

\definecolor{ltblue}{rgb}{0,0.4,0.4}
\definecolor{dkblue}{rgb}{0,0.1,0.6}
\definecolor{dkgreen}{rgb}{0,0.35,0}
\definecolor{dkviolet}{rgb}{0.3,0,0.5}
\definecolor{dkred}{rgb}{0.5,0,0}

\begin{document}

\lstset{language=coq, basicstyle=\ttfamily\scriptsize, columns=flexible,
keepspaces=true}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Formalizing the Honeynet}
\subtitle{Defining and Proving a Rootkit Installation}

\authorinfo{CM Lubinski}
           {DePaul University}
           {cm.lubinski@gmail.com}

\maketitle

\begin{abstract}
Formal definitions of multiple file formats, ``malicious'' file names, and
event time-lines are provided in the proof-oriented language, Coq. These are
combined to mimic the types of evidence given by independent forensics
researchers in a forensics competition (``Honeynet''). Using the definitions,
the evidence is proven to be consistent with provided disk images. Along the
way, functions for parsing and manipulating relevant data structures within
Coq are also described.
\end{abstract}

\keywords
forensics, formalization, Coq, Honeynet, Ext2

\section{Introduction}

After completing a lengthy customs process in the largest airport of
burgeoning global power, you notice that your laptop appears to be running
rather slowly. You suspect that malicious software has been installed by the
country's authorities, but are uncertain how to prove this. At first, you
consider using off-the-shelf antivirus software to find the infection, but
realize that the risk of damaging international relations is far too great to
rely on the definitions within the proprietary software. What you really need
is a concrete, standardized definition of what it means to have malware
installed and evidence fitting that definition to prove that your machine has
been invaded.

This scenario and others like it (though, perhaps more mundane) are the
ultimate goal of the research provided in this paper. To achieve that, we need
formal definitions for various types of evidence needed by forensics analysts.
We also need a way to convert real data (e.g. from a disk image) into these
definitions such that we can provide concrete {\em proofs} showing that a
particular image does or does not fit such a definition. In the above example,
we could provide a {\em proof} that the disk image satisfied a definition of
``malware installation''. 

To guide our search, we will focus on the evidence structures and proofs
described by several independent researchers studying a ``Honeynet''
challenge. The Honeynet Project\cite{honeynet}'s now-defunct {\it Scan of the
Month} series provided researchers a disk image attained from a compromised
honeypot (a computer created with the explicit goal of catching malware for
inspection.) Each month, participants were challenged to describe what
happened to the system and provide evidence for their conclusions. We will
consider one specific contest\cite{honeynet-15}, in which a rootkit
(replacement system programs that hide malicious activity) was installed on a
server. The security community was asked to recover the rootkit, prove that it
had been installed, and provide a step-by-step writeup describing how the
rootkit was found.

We will describe our definitions, code our parsers, and implement our evidence
in the proof-oriented language, Coq. While this language lives in the
ML-family of programming languages, we will augment it with several functional
and object-oriented notations. Using Coq allows us to formalize our proofs and
definitions, but significantly restricts the practicality of our efforts; we
will expand on these limitations throughout the paper.

By the end of this write-up, we will have provided definitions used by some of
the Honeynet contestants. These include definitions of tar files,
malicious-looking file names, file deletion (on the Ext2 file system), as well
as a time line of events which would be consistent with rootkit installation.
Given portions of the same disk provided to those researchers, we will also
present proofs that it satisfies these definitions.

\section{Getting Our Feet Wet with File Types}

We start by considering a relatively straight-forward request: defining what
it means for a given file to be a JPEG. How can we formalize this notion? One
tact (used by many operating systems) is to rely on the file extension (if
present) -- in this case, checking for either {\tt jpg} or {\tt jpeg}. This is
a very loose definition, however, as malicious users need only give their
files a different extension to avoid detection. We could instead review the
JPEG spec and confirm that all of the meta data contained within this file is
consistent with said spec. This approach runs the opposite end of the
spectrum, requiring significantly more evidence. Further, the JPEG spec is not
as tidy as we might hope; most applications are lenient with the file formats
they accept and messy with the files they write.

Instead, we will chose a middle route, opting to use ``magic numbers'' as our
guide. This refers to tell-tale byte values at predictable offsets within the
file data. These file signatures are {\it relatively} unique to various file
formats, so we will use them in our definitions and add additional checks as
necessary. JPEGs happen to always start with the bytes {\tt ff d8} and end
with {\tt ff d9}. Similarly, gzipped files begin with {\tt 1f 8b 08} and Linux
executables (ELFs) begin with {\tt 7f 45 4c 46} ({\tt 7f} E L F).

If we represent each byte as a positive integer, then writing these
definitions in Coq (plus some Pythonic syntactic sugar) would look like

\begin{lstlisting}
Definition isJpeg (file: File) :=
     file @[  0 ] = value 255
  /\ file @[  1 ] = value 216 
  /\ file @[ -2 ] = value 255
  /\ file @[ -1 ] = value 217.

Definition isGzip (file: File) :=
     file @[  0 ] = value 31
  /\ file @[  1 ] = value 139 
  /\ file @[  2 ] = value 8.

Definition isElf (file: File) :=
     file @[  0 ] = value 127
  /\ file @[  1 ] = value 69 
  /\ file @[  2 ] = value 76
  /\ file @[  3 ] = value 70.
\end{lstlisting}

For each definition, the first (and, in the case of the JPEG, last) bytes of a
file must both be present (as indicated by {\tt value}) and equal to the
unsigned byte values shown. We will discuss the absence of byte values later,
but for now, assume that this accounts for ``missing'' evidence which might
arise if a disk were damaged or inconsistent. By defining file types in this
manner, we can use these definition as building blocks within larger
definitions and within proofs. For example, we can now {\it prove} that JPEG
files cannot also be gzipped files:

\begin{lstlisting}
Lemma jpeg_is_not_gzip : forall (file: File),
  (isJpeg file) -> ~(isGzip file).
\end{lstlisting}

A full version of this proof (and others mentioned throughout this paper) is
provided in the appendix.

\section{Bird's Eye View of a Honeynet Proof}

Let's now consider the Honeynet {\it Scan of the Month} mentioned in the
introduction. In his entry for this contest, Matt 
Borland\cite{borland-honeynet} described a deleted, ``tar/gzipped file
containing the tools necessary for creating a home for the attacker on the
compromised system''. Formalizing this a bit, we will say that he proved that

\begin{lstlisting}
Lemma borland_honeynet_file:
  exists (file: File),
  (isOnDisk file honeynet_image_a)
  /\ isDeleted file
  /\ isGzip file
  /\ exists (filename: ByteString),
     (In filename (Tar.parseFileNames (gunzip_a file)))
     /\ looksLikeRootkit filename.
\end{lstlisting}

That's a bit of a mouthful; we are stating that there exists a deleted,
gzipped file on the Honeynet disk image such that, when we unzip that file,
the contained tar includes a malicious-looking file name. For the remainder of
this section, we will dive deeper into each of the definitions in the ``and''
clause (save {\tt isGzip}, which we have already described.) Section 5 will
investigate this evidence even closer, and include a description of the
assumptions (as indicated by {\tt \_a}.)

\subsection{isOnDisk}

How might we define that a file "exists" on a particular disk image? For a
simple definition, we could start by claiming a ``file'' is on a disk image if
that file's contents could be found sequentially on the disk. In other words,
we might consider the file to be on disk if we can find a starting index on
the disk such that every byte afterwards matches that of the file.

\begin{lstlisting}
Definition isOnDiskTry1 (file: File) (disk: Disk):
  exists (start: Z), forall (i: Z),
  (i >= 0 /\ i < file.(fileSize)) -> 
    file @[ i ] = disk (start + i).
\end{lstlisting}

This definition isn't very useful in practice, however, as files are very
often fragmented across multiple segments of a disk image. Moreover, this
definition includes many false positives, where a ``file'' is formed by
looking at the bits that span a fragment boundary. Files are stored on disks
via file systems, which make no sequential guarantees (particularly in recent
file systems such as ZFS and Btrfs). On spinning hard drives, the need to
``defragment'' arises from the segmentation of files over non-adjacent sectors
on disk; moving the fragments to be adjacent improves read performance by
reducing the times the drive head must skip around.

As sequential access will not do, we must build our definition for file
existence with file systems in mind. Proving that a file exists within each
type of file system is a relatively unique operation, however, so we will
define file existence as the disjunction of file existence within each file
system. While the definitions are distinct, they each tend to follow the
pattern that there exists some file identifier such that if we were to parse
the file associated with that identifier on the given disk, we would find a
file identical to the provided file.

\begin{lstlisting}
Definition isOnDisk (file: File) (disk: Disk):
  (* Ext2 *)
  (exists (inodeIndex: Z):
    fileEq (Ext2.findAndParseFile disk inodeIndex)
           (value file))
  \/ (* FAT32 *)
  (exists (clusterNumber: Z):
    fileEq (Fat32.findAndParseFile disk clusterNumber) 
           (value file))
  \/ (* Btrfs *)
  (exists (key: Z):
    fileEq (Btrfs.findAndParseFile disk key)
           (value file))
  \/ ...
.
\end{lstlisting}

The functions {\tt findAndParseFile} perform the work of inspecting the disk
image, reading the relevant data structures, and creating an abstract
representation of the results (in this case, a File object.) We will discuss
parsing computations a bit later and {\tt fileEq} in the next section.

\subsection{The File Structure and isDeleted}

Like existence on the disk, what it means for a file to be deleted is quite
file system specific. In Ext2, a file might be seen as deleted if its bit in
the allocation bitmap were zero; in FAT32, we might consider a file deleted if
it were not accessible via a cluster chain @TODO: Confirm. To account for
these varieties, we will include the deletion status of a File within its {\em
definition}, allowing each file system to define the status how it wishes.
This definition attempts to describe ``universal'' file truths:

\begin{lstlisting}
Structure File := mkFile {
  fileSize: Z;
  deleted: bool;
  byteOffset: ByteData
}.
\end{lstlisting}

The first two fields are self-explanatory, but the third warrants additional
review. This field is a function that, when given an offset within a file,
returns the byte value found at that offset within the file. As described in
our investigation of file types, we must account for the possibility that the
byte requested is not available (e.g. out of range or not within our
assumptions), so we must wrap the result in an Exc (a.k.a. Option).

\begin{lstlisting}
Definition ByteData := Z -> Exc Z.
\end{lstlisting}

Given the definition of Files, we can describe {\tt fileEq}, our first
work-around required by our language choice. We cannot rely on native equality
because we want the {\tt byteOffset} field to be flexible enough to allow for
complex, partially applied functions. While we want to define equality of such
functions as pairwise mappings between input and output, Coq would attempt to
normalize them before comparing. The functions we will use are too large (when
normalized) for Coq to handle in a reasonable time frame. Instead, we will use
a custom relationship, {\tt fileEq}, which operates on two {\tt Exc File}s,
and holds if the simple fields match and if, for all file offsets, the bytes
returned by {\tt byteOffset} match.

\begin{lstlisting}
Definition fileEq (lhs rhs: Exc File) :=
  match (lhs, rhs) with
  | (error, error) => True
  | (value lhs, value rhs) =>
      lhs.(fileSize) = rhs.(fileSize)
      /\ lhs.(deleted) = rhs.(deleted)
      /\ forall idx:Z, lhs @[ idx ] = rhs @[ idx ]
  | _ => False
  end.
\end{lstlisting}

Coming back to goal, it should be easy to see that the definition of {\tt
isDeleted} effectively just delegates to the boolean value that is part of the
File representation:

\begin{lstlisting}
Definition isDeleted (file: File) :=
  file.(deleted) = true.
\end{lstlisting}

\subsection{Malicious Filenames}

The final clause in our lemma states that one of the filenames contained in
the gzipped-tar is suspicious. We will describe the {\tt gunzip\_a} assumption
shortly; for now, know that it is a representation of a decompression
algorithm for gzipped files. Once the file is unzipped, we run a parsing
computation, {\tt Tar.parseFileNames}, which retrieves a list of all of the
file names contained within the unzipped tar.

The only new {\it definition} is that of {\tt looksLikeRootkit}. We will say
that a filename looks like one involved in a rootkit if it, excluding its path
prefix, matches any of a predefined list of system files. These include task
managers such as {\tt top} and {\tt ProcMon.exe}, which rootkits replace to
hide their activities, as well as {\tt ssh} and {\tt rsync}, which grants them
the ability to monitory network traffic.

\begin{lstlisting}
Definition ByteString := list Z.

Definition looksLikeRootkit (fileName: ByteString):
  In (trimFileNamePrefix fileName) 
     (map ascii2Bytes ("ps" :: "netstat" :: "top" :: "ifconfig" 
                       :: "ssh" :: "rsync" :: "ProcMon.exe" 
                       :: nil)).
\end{lstlisting}

We treat file names as lists of bytes to account for unicode and other
non-ASCII characters which would be excluded by Coq's string/ascii package.
For the ease of description, however, we write ``malicious-looking'' file
names in ASCII, and convert them all into ByteStrings via the {\tt
ascii2Bytes} function, described in section 5.5

Use of this definition is straight forward; full proof tactics are available
in the appendix.

\begin{lstlisting}
Lemma rootkit_ex1 : 
  looksLikeRootkit (ascii2Bytes "last/top").

Lemma rootkit_ex2 : 
  ~(looksLikeRootkit (ascii2Bytes "last/example.txt")).
\end{lstlisting}

\subsection{Summary}

We have now described (at a high level) {\tt isOnDisk}, {\tt isDeleted},
{\tt isGzip}, and {\tt looksLikeRootkit} enough that we could see why Borland
proposed them as definitions of evidence for a rootkit installation. This is
not complete, however, as we skipped over several critical parsing and
computation functions which we will expand on in section 5. Before describing
these functions, we should clarify the relationships between some of the
concepts we are proposing; this is the focus of the next section.

\section{Lemmas, Computations, Definitions; Oh My!}

We have now come across several core concepts of this research, and it is easy
to see how they might be confused. Before we continue, let us solidify our
understanding of each category.

We are most familiar with {\bf definitions}, which provide a name for a common
understanding of a forensics concept. For example, we provided definitions for
{\tt isGzip}, {\tt looksLikeRootkit}, etc. While one of the wider goals of
this research is to provide a set of such common definitions, this paper only
describes several which are relevant to the Honeynet example. Other
definitions might include ``web page access'' (e.g. by inspecting browser
history), ``in contact with'' (e.g. there exists records of email
communication), and ``last time logged in.'' Note that definitions are, at
their core, the choice of their creators; forming a consensus on a definition
is the only way it may provide authority. This also means that the possibility
for multiple definitions is present (as we have shown with our two definitions
for file existence.)

Definitions are most often built by combining aspects of various abstract {\bf
data structures}, ``universal'' representations of data-related concepts
within forensics. We have seen these used to represent files, and we will see
them represent various Ext2 file system structures, as well as events in a
reconstructed timeline. These structures serve as a way of differentiating
particular parsing computations (which build the structures) from definitions
involving these structures. It should be relatively simple for someone to
write a different parsing algorithm which produces the same structures without
affecting any of the definitions.

Unfortunately, the Coq proof environment is quite limited with regards to
practical applications. It cannot, for example, read bytes from a disk image;
it cannot (for our intents) call external programs to transform data; it
cannot even instantiate a list of tens of thousands of values. We work around
these limitations by making {\bf assumptions} about our environment (with the
suffix {\tt \_a}) which entail only the relevant pieces of data needed for our
proofs. We won't read a file from disk; instead, we will generate a {\it
sparse} map to represent that disk, including only the offsets we care about.
Assumptions are also used to account for transformations that would normally
be performed by a trusted, external program. We cannot, for instance, run
``gunzip'', nor do we want to try to implement that algorithm within Coq.
Instead, we require an assumption be made that encompasses that activity. Our
proofs cannot validate that {\tt disk\_a} or {\tt gunzip\_a} accurately
represent values on the disk or the output of an unzipping operation -- they
instead trust those assumptions to be accurate and rely on users for
verification.

Where possible, we do attempt to {\bf parse} and/or {\bf compute} values
within Coq. Here we have codified algorithms which take the bytes from disk
(assumptions) and convert them into relevant structures. This might mean
creating Inodes, SuperBlocks, and GroupDescriptors for a disk with Ext2 on it,
or reading headers from the data associated with an abstract tar file. In all
cases, these operations are our (perhaps flawed) implementations of various
specifications; users should feel free to check the structures created against
those found using other tools. Ultimately, we aimed to provide definitions and
data structures which could serve as building blocks for proofs; parsing and
computations are simply the glue that tie those building blocks to actual
data.

The Honeynet competition we've described required researchers to provide
``{\bf evidence}'' or ``proof'' that the disk had been compromised by a
rootkit. This required that the researchers both define what acceptable
evidence would look like as well as provide it the evidence regarding the
attacked server. This led each researcher to provide different types of
evidence depending on what that researcher found to be acceptable. While we
could create definitions for the various types of evidence, we will instead
follow the Honeynet applicants' lead and provide only the evidence itself in
the form of a {\bf lemma} or proof. This evidence will be in the form of a
proof term, which the author of the proof can build up using Coq's tactic
system. It is then up to the reviewer to determine whether or not the evidence
provided by an applicant fits a familiar definition.

Finally, the code written for this research is sprinkled with {\bf utility
functions} to perform (mostly) simple transformations. {\tt
trimFileNamePrefix} and {\tt ascii2Bytes} are two such examples which make
explaining (and hopefully, understanding) the code samples easier. These
functions are not necessary for the definitions, proofs, etc. where they
appear; they simply make the code more terse.

\section{Computing Our First Lemma}

Not that we have introduced each of these concepts, let's step through our
{\tt borland\_honeynet\_file} proof, which will touch on each.

\subsection{A Note on Functional Idioms}

Particularly while parsing, we will make use of several idioms from functional
programming. In particular, we make heavy use of the ``option'' pattern; as
the parsing code does not know whether bytes will be {\em present} on the
provided disk image, each attempt to read returns an {\em option} of the
result. An option is simply a wrapper around a value such that the wrapper may
be contain the value (indicated by {\tt value} or {\tt Some}), or may not
contain the value (indicated by {\tt error} or {\tt None}). This is why the
return type of many functions is {\tt Exc A}; this indicates an option of type
{\tt A}.

Options make error handling ``percolate up'' in that functions which produce
options can be chained such that any error results in the entire computation
being an error. To get to this point be define two functions, indicated by the
infix notation {\tt \_map\_} and {\tt \_flatmap\_}. The function signatures
for each should aid their explanation.

\begin{lstlisting}
Definition opt_map {A B: Type} (opt: Exc A) (fn: A -> B)
  : Exc B.

Definition flatmap {A B: Type} (opt: Exc A) (fn: A -> Exc B)
  : Exc B.
\end{lstlisting}

The role of {\tt \_map\_}, then, is to transform the contents of an option. If
the option is empty, map has no effect. {\tt \_flatmap\_} similarly does not
affect empty options, but performs a bit differently with {\tt values}.
Instead of simply transforming the contents, the entire option gets replaced
with the result of {\tt fn}, allowing sequences like

\begin{lstlisting}
  (file @[ 0 ]) _flatmap_ (fun byte0 =>
  (file @[ 1 ]) _flatmap_ (fun byte1 =>
  (file @[ 2 ]) _map_ (fun byte2 =>
    (byte0, byte1, byte2)
  )))
\end{lstlisting}

which can return either {\tt error} or {\tt value } with all three bytes
present. If the first byte was not present, the outer-most {\tt \_flatmap\_}
would not have executed the inner function. Similarly, if the second byte were
not present, the function including {\tt \_map\_} would not be ran, and if the
third byte were unavailable, the function with the parameter {\tt byte2} would
not be evaluated.

We've rushed through these concepts as we assume the reader has some
background in functional programming. For a more thorough (yet brief and
practical) introduction, see Wamper \& Miller\cite{scala}.

\subsection{Assumptions}
Our proof will make use of two assumptions: {\tt honeynet\_image\_a}, a {\tt
Disk} and {\tt gunzip\_a}, a function, {\tt File->File}. The {\tt Disk} type
is used to represent a disk image throughout our definitions and proofs.
Technically, they are functions from {\tt Z} to {\tt Exc Z}, acting as
byte-retrieval mechanism; give a {\tt Disk} an offset, and it will respond
with the byte value (a {\tt Z}) at that offset in the disk image, or {\tt
error} if no such byte exists on the disk. Generally, disk data is backed by
an in-memory mapping via Coq's {\tt FMapAVL} trees, and must be generated from
the true disk values. Due to Coq's limitations on data structure sizes, we
only populate these maps with essential values; once the map reaches a few
thousand entries, it will no longer be usable within Coq.

The second assumption represents gzip's unzipping/deflating operation. As
implementing the decompression algorithm used within Coq would not provide
particularly valuable information, we instead delegate its operation to an
assumption. This assumption needs to be able to convert a compressed, gzip
file into the corresponding, uncompressed file, which effectively means
providing a new {\tt File} populated with the relevant bytes. In our example,
we used the {\it Sleuth Kit}'s {\tt icat} program to pull inode {\tt 23}'s
contents from the disk image, ran {\tt gunzip} on the resulting {\tt tgz}
file, and then pulled the relevant bytes (i.e. those necessary from the tar
file headers). We effectively just wrap this data in a function that,
regardless of {\tt File} input, returns the uncompressed, tar {\tt File}.


\subsection{Parsing a File via INode}

Our evidence relies on the {\it existence} of a {\tt File} which is on disk,
deleted, etc., so our proof for this evidence need only provide such a {\tt
File}. Hypothetically, this should be easy, as we can just pull the file from
the disk image directly. We therefore use a parsing function (rather, a series
of functions) to retrieve this file and provide it as demonstrative proof.
Ultimately, we will need to call

\begin{lstlisting}
(Ext2.findAndParseFile honeynet_image_a 23)
\end{lstlisting}

To be confident in this function's results, however, we will need to
understand how Ext2 file systems are laid out on disk. We will proceed by
pealing off each layer of the call and review the constructed data structures.

\subsubsection{findAndParseFile}

At the outermost conceptual layer, we have a function which, when given a disk
and Ext2 Inode identifier, returns either an error or a valid File structure
associated with that file. As we discussed earlier, a {\tt File} is composed
of a deletion status, a file size, and a function that retrieves bytes within
the file. These fields require we first parse out the {\tt SuperBlock} of the
disk and the {\tt GroupDescriptor} and {\tt Inode} associated with our index
({\tt 23}).

To understand what these structures mean, one must first learn how Ext2 is
structured. At its core, the file system is composed of a sequence of
equally-sized chunks of bytes (called ``blocks''). Files are not necessarily
stored on sequential (or even contiguous) blocks; their data may be parcelled
throughout the disk image (as we will describe when discussing Inodes, below).
Each file is referenced by a single ``Inode'' structure, which keeps track of
the file's data locations as well as access times, file size, and other meta
data. Inodes are collected into ``groups'', which have meta data stored in a
``Group Descriptor''. 

This descriptor includes information about which Inodes are allocated, etc. as
well as provides a mechanism to segment the administration of Inodes. We can
easily compute which group a particular Inode index belongs to by dividing it
by the number of inodes per group. We can compute the Inode's position within
that group by taking the remainder of this division (i.e. by applying the
mod). It's important to note for both of these operations that Inode indices
are one-indexed, as are group descriptors; this will lead to some additions
and subtractions of one throughout our code to convert between zero- (which is
easier to compute on) and one-based indices.

Meta data about the file system as a whole is stored in a special block known
as the ``SuperBlock'', which lives at a predictable position on the disk. The
SuperBlock contains information such as how large each block is, where the
collection of GroupDescriptors starts, and the number of Inodes per group.
While we do not use this fact, the SuperBlock is usually stored redundantly on
the disk, meaning we could verify its values with the copies.

Returning to our parsing efforts we must note that each attempt to pull out a
SuperBlock, GroupDescriptor, etc. may fail, and if this occurs, we want the
entire File parsing function to propagate the failure. Here we will use the
{\tt \_flatmap\_} approach described above, treating the computation as a
monad.

\begin{lstlisting}
Definition findAndParseFile (disk: Disk) (inodeIndex: Z) 
  : Exc File :=
  (findAndParseSuperBlock disk) _flatmap_ (fun superblock =>
  let groupId := ((inodeIndex - 1) (* One-indexed *)
                  / superblock.(inodesPerGroup)) + 1 in
  let inodeIndexInGroup := 
    (inodeIndex - 1) mod superblock.(inodesPerGroup) in
  (findAndParseGroupDescriptor disk superblock groupId) 
    _flatmap_ (fun groupdesc =>
  (findAndParseInode disk superblock groupdesc inodeIndex) 
    _flatmap_ (fun inode =>
  (parseDeleted disk superblock groupdesc inodeIndex) 
    _map_ (fun deleted =>
    mkFile
      inode.(size)
      deleted
      (fetchInodeByte disk superblock inode)
  )))).
\end{lstlisting}

\subsubsection{findAndParseSuperBlock}

Regardless of block size, the first SuperBlock can be found starting at the
1024th byte. Below this position is the boot sector, executable code that
loads prior to the main operating system (think boot loaders like LILO, GRUB,
and MBR). While the size of the SuperBlock depends on the revision of Ext, the
spec is largely compatible, so we will use the SuperBlock structure described
by one of Ext2's original authors, David Poirier\cite{non-gnu}. This spec
cares only about the first 264 or so bytes of the block (regardless of its
size).

Finding and parsing the SuperBlock, then, amounts to jumping to offset 1024 on
the disk and plugging in the read values into a SuperBlock structure. To make
our lives a tad easier, we will use a {\tt shift} function, which effectively
shifts the beginning of a disk by serving as a layer of indirection when
pulling bytes from the disk. This wrapper allows us to parse as if the 1024th
index were at position zero.

The values we need to store to construct a SuperBlock are largely encoded as
little endian, 4-byte integers. To parse this sequence of bytes into a more
usable integer, we will make frequent use of an unsigned conversion function,
{\tt seq\_lendu}. This function simply reads in a sequence of bytes from the
disk starting at a position (as given by the second parameter) and running for
a specific length (as given by the third), and converts that into an integer
based on little endian semantics. Note that in this code sample, we omit most
of the SuperBlock's fields (there are roughly 45) as their addition should be
clear.

\begin{lstlisting}
Definition findAndParseSuperBlock (disk: Disk)
  : Exc SuperBlock :=
  let disk := (shift disk 1024) in
  (seq_lendu disk 0 4) _flatmap_ (fun inodesCount =>
  (seq_lendu disk 4 4) _flatmap_ (fun blocksCount =>
  (seq_lendu disk 8 4) _flatmap_ (fun rBlocksCount =>
  (* ... Additional fields omitted ... *)
  (seq_lendu disk 260 4) _map_ (fun firstMetaBg =>
    mkSuperBlock
      inodesCount
      blocksCount
      rBlocksCount
      (* ... Additional fields omitted ... *)
      firstMetaBg
  ))))))))))))))))))))))))))))))))))))))))))))).
\end{lstlisting}

While we parse out virtually all of the fields of a SuperBlock, we will need
only a handful. {\tt inodesCount} provides an upper bound for inode indices
(which we use to validate that a given inode exists). {\tt inodesPerGroup}
will appear several times in this paper; the field indicates the number of
inodes assigned to each GroupDescriptor. Hence, that field provides a way to
determine which GroupDescriptor is needed for a particular inode index. The
{\tt logBlockSize} field is also of interest, as we will use it to determine
the number of bytes each block on disk spans. This is encoded using the
logarithmic scale, but we provide a simple function to convert to the true
number of bytes.

\begin{lstlisting}
Definition blockSize (superblock: SuperBlock) := 
  Z.shiftl 1024 superblock.(logBlockSize).
\end{lstlisting}

\subsubsection{Block Addresses}

As mentioned earlier, the Ext2 file system breaks the disk into ``blocks'' for
reference purposes; these are akin to cylinders in the FAT32 file system. Due
to locality of reference, in practice, the need to retrieve a single byte from
a disk is very rare. Instead, data is most often retrieved in sequential
chunks, which maps well to the concept of data blocks.  Depending on cache
size, whole blocks are read at a time. These blocks are identified by their
one-indexed ``block address'', signified by the type, BA. 

To find the initial byte of a block based on its address alone, we need to
first find the size of each block (defined in the SuperBlock.) With that, we
can treat the disk as an array of block and simply jump to the relevant
position.

\begin{lstlisting}
Definition ba2Offset (superblock: SuperBlock) (blockAddress: BA)
  := (blockSize superblock) * blockAddress.
\end{lstlisting}

\subsubsection{findAndParseGroupDescriptor}
An array of GroupDescriptors can be found in the block following that which
contains the SuperBlock. Depending on block size, that may mean the
GroupDescriptors begin at block one if the block size is greater than 1024
(and hence, the SuperBlock is part of block zero) or at block two otherwise
(when the SuperBlock composes all of block one). 

GroupDescriptors will be represented by a structure similar to SuperBlocks,
though with far fewer fields. GroupDescriptors have 32 allocated bytes, though
only 20 are used. With this fact and knowledge of where the group descriptor
array starts, we can jump to a particular GroupDescriptor by multiplying the
structure's size by the index we seek. This formula needs to be modified
slightly to account for one-based indexing, but otherwise the parsing is quite
straight forward. As with the SuperBlock, we will shift the disk to aid our
parsing efforts. 

\begin{lstlisting}
Definition findAndParseGroupDescriptor 
  (disk: Disk) (superblock: SuperBlock) (groupId: Z)
  : Exc GroupDescriptor :=
  let groupBlockArrayBA := if (blockSize superblock =? 1024)
    then 2 else 1 in
  let groupBlockArrayOffset := 
    ba2Offset superblock groupBlockArrayBA in
  (* groupId is one-indexed *)
  let descriptorOffset := 32 * (groupId - 1)  in
  let disk := (shift disk (groupBlockArrayOffset 
                           + descriptorOffset)) in
  (seq_lendu disk 0 4) _flatmap_ (fun blockBitmap =>
  (seq_lendu disk 4 4) _flatmap_ (fun inodeBitmap =>
  (seq_lendu disk 8 4) _flatmap_ (fun inodeTable =>
  (seq_lendu disk 12 2) _flatmap_ (fun gdFreeBlocksCount =>
  (seq_lendu disk 14 2) _flatmap_ (fun gdFreeInodesCount =>
  (seq_lendu disk 16 2) _flatmap_ (fun usedDirsCount =>
    mkGroupDescriptor
      blockBitmap
      inodeBitmap
      inodeTable
      gdFreeBlocksCount
      gfFreeInodesCount
      usedDirsCount
  )))))).
\end{lstlisting}

Two of the fields from this structure will be particularly useful, {\tt
inodeBitmap} and the {\tt inodeTable}. Both contain block addresses, pointing
to the start of a sequence of inode-related data. The former is simply an
array where each bit represents whether or not the corresponding Inode is
allocated; this will be of great use when we are calculating deletion status.
The latter points to an array of Inode structures, as described in the next
section.

\subsubsection{findAndParseInode}

Ext2 tracks meta information about specific files through ``Inode''
structures. One such data structure exists for each file and contains copious
information relevant to our interests, including creation time, deletion time,
and references to the data blocks which make up this file. As each file has a
unique Inode within the Inode array, we can refer to files by their {\it Inode
Index}, which we do throughout this paper.

GroupDescriptors contain a reference to the block address associated with the
start of the Inode array. From there, we can pin point the beginning of the
relevant Inode structure by calculating the Inode's position within the block
group the same way we calculated GroupDescriptors -- each Inode is 128 bytes.
Add in a one-based offset and a check that the requested Inode is valid and
you have the {\tt findAndParseInode} function.

\begin{lstlisting}
Definition findAndParseInode (disk: Disk) 
  (superblock: SuperBlock) (groupdesc: GroupDescriptor)
  (inodeIndex: Z) : Exc Inode :=
  (* Check for valid Inode *)
  if (inodeIndex >=? superblock.(inodesCount))
  then error
  else
    (* Inode Table is 1-indexed *)
    let inodeIndexInTable := 
      ((inodeIndex - 1) mod superblock.(inodesPerGroup)) in
    let inodePos := (ba2Offset superblock
                               groupdesc.(inodeTable))
                     + (inodeIndexInTable * 128) in
    let disk := (shift disk inodePos) in
    (seq_lendu disk 0 2) _flatmap_ (fun mode =>
    (seq_lendu disk 2 2) _flatmap_ (fun uid =>
    (* ... Additional fields omitted ... *)
    (seq_lendu disk 40 4) _flatmap_ (fun directBlock1 =>
    (seq_lendu disk 44 4) _flatmap_ (fun directBlock2 =>
    (* ... Additional direct blocks omitted *)
    (seq_lendu disk 84 4) _flatmap_ (fun directBlock12 =>
    (seq_lendu disk 88 4) _flatmap_ (fun indirectBlock =>
    (seq_lendu disk 92 4) _flatmap_ (fun doubleIndirectBlock =>
    (seq_lendu disk 96 4) _flatmap_ (fun tripleIndirectBlock =>
    (* ... Additional fields omitted ... *)
    (seq_lendu disk 116 4) _map_ (fun osd2 =>
      mkInode
        mode
        uid
        (* ... Additional fields omitted ... *)
        (directBlock1 :: directBlock2 :: directBlock3
          :: directBlock4 :: directBlock5 :: directBlock6
          :: directBlock7 :: directBlock8 :: directBlock9
          :: directBlock10 :: directBlock11 :: directBlock12
          :: indirectBlock :: doubleIndirectBlock
          :: tripleIndirectBlock :: nil)
        (* ... Additional fields omitted ... *)
        osd2
    )))))))))))))))))))))))))))))))).
\end{lstlisting}


\subsubsection{parseDeleted}
With the Inode, we know the first attribute of our File object, the file size.
We next turn to the second field, whether or not the file is marked as
deleted, or ``unallocated''. To find the allocation status of an Inode, we'll
need to find the associated allocation bitmap (a sequence of bits on disk such
that zero indicated unallocated and one indicates allocated.) The bitmap
associated with a particular Inode is indicated in that Inode's
GroupDescriptor. As we are reading entire bytes at once, we'll read the byte
which contains the allocation bit for the Inode we care about and then test
the bit within the read byte.

Ultimately, this means we will find the start of the allocation bitmap from
the GroupDescriptor, jump to the byte within that bitmap which contains the
allocation of our Inode, and then test the bit within that byte. As noted
before, we need to be weary of the fact that Inodes are one-indexed.


\begin{lstlisting}
Definition parseDeleted (disk: Disk) (superblock: SuperBlock)
  (groupDesc: GroupDescriptor) (inodeIndex: Z) : Exc bool :=
  let inodeIndexInGroup := 
    (* 1-Indexed *)
    (inodeIndex - 1) mod superblock.(inodesPerGroup) in
  let bitmapStart := 
    ba2Offset superblock groupDesc.(inodeBitmap) in
  (* Fetch the allocation byte for this inode *)
  (disk (bitmapStart + (inodeIndexInGroup / 8))) 
    _map_ (fun allocationByte =>
    (* The bit associated with this inode is 0 *)
    (negb (Z.testbit allocationByte 
                     (inodeIndexInGroup mod 8)))
  ).
\end{lstlisting}

\subsubsection{fetchInodeByte}
With the deletion status and file size, we need only more more attribute to
create our File structure. File objects have a method which, given an
arbitrary offset within the File, returns the associated byte within the File,
effectively linearizing byte access. This abstraction is quite necessary as
each File system will have a different method of accessing the File bytes. We
now discuss how such a method works on Ext2.

Ext2 was designed with a trade-off in mind; quick access to file data
generally runs counter to the desire to store very large files. The solution
the file system employs is to allow the first twelve data blocks to be
directly addressable from the Inode. Slightly larger files can be addressed
through an indirection pointer, which points to a block which in turn points
to several blocks of data. Even larger files can be addressed through a
double-indirection pointer, which points to a block which points to other
blocks which each contain references to additional data blocks. There is also
a third level of indirection, allowing Ext2 to address up to roughly sixteen
million data blocks.

This means that determining which block a given byte within a file is located
in may require walking one or more indirection blocks. As this is a recursive
operation (depending on the level of indirection,) we next describe a fixpoint
which will pull out the block address associated with a given byte. This
function checks if it has reached the final level of indirection, as indicated
by the {\tt O} (Coq's zero) case; if so, it knows to grab the correct block
address (a four byte value) from the block of pointers.

If the function has not reached its base case ({\tt S}, the ``successor of''),
it determines which of its block addresses to follow, reduces the block number
requested accordingly, and recursively tries again. To select the correct
block address to recursively follow, we must determine how many blocks each
indirection pointer is responsible for ({\tt unitSizeInBlocks}), and select
the index of the block address within the array accordingly ({\tt
nextBlockIndex}). Using that, and the fact that each address is four bytes
large, we jump to the correct offset within the pointer array, read those four
bytes, and recurse accordingly.

\begin{lstlisting}
Fixpoint walkIndirection (disk: Disk) (superblock: SuperBlock)
  (blockNumber indirectionPos: Z) (indirectionLevel: nat) 
  : Exc Z :=
  match indirectionLevel with
  | O => 
    let bytePosition := (indirectionPos + 4 * blockNumber) in
    (seq_lendu disk bytePosition 4)
  | S nextIndirectionLevel =>
    let exponent := Z.of_nat indirectionLevel in
    let unitSizeInBlocks := 
      ((blockSize superblock) ^ exponent) / (4 ^ exponent) in
    let nextBlockIndex := blockNumber / unitSizeInBlocks in
    let nextBytePosition := 
      indirectionPos + 4 * nextBlockIndex in
    (seq_lendu disk nextBytePosition 4) 
      _flatmap_ (fun nextBlockBA =>
      walkIndirection disk superblock 
                      (blockNumber mod unitSizeInBlocks)
                      (ba2Offset superblock nextBlockBA)
                      nextIndirectionLevel
      )
  end.
\end{lstlisting}

Building on top of this fixpoint, we can define a single function that, when
given a Disk, SuperBlock, Inode and offset, returns the byte within the
associated file at that offset. We destruct the function in to three parts.
First, we check if the byte requested is within the file.

\begin{lstlisting}
Definition fetchInodeByte (disk: Disk) (superblock: SuperBlock)
  (inode: Inode) (bytePos: Z): Exc Z :=
  if inode.(size) <=? bytePos then 
    error
  else 
    ...
\end{lstlisting}

Next, we determine the block address using the recursive function we described
above. Depending on the byte position requested, the block address which of
the data block which will contain it may be either directly addressable (in
which case we grab the address from the Inode's list), or we may need to use
one of the indirection blocks.
\begin{lstlisting}
    ...
    let blockSize := (blockSize superblock) in
    let blockNumInFile := bytePos / blockSize in
    let directAddressable := 12 in
    let indirect1Addressable := blockSize / 4 in
    let indirect2Addressable := (blockSize * blockSize) / 16 in

    let blockAddress :=
    (if blockNumInFile <? directAddressable then
      nth_error inode.(block) (Z.to_nat blockNumInFile)

     else if blockNumInFile <? directAddressable
                               + indirect1Addressable then
      (nth_error inode.(block) 12) 
        _flatmap_ (fun indirectBlock =>
        walkIndirection disk superblock
          (blockNumInFile - directAddressable)
          (ba2Offset superblock indirectBlock) 
          O
        )

    else if blockNumInFile <? directAddressable
                              + indirect1Addressable
                              + indirect2Addressable then
      (nth_error inode.(block) 13) 
        _flatmap_ (fun doubleIndirectBlock =>
        walkIndirection disk superblock 
          (blockNumInFile - directAddressable
                          - indirect1Addressable)
          (ba2Offset superblock doubleIndirectBlock)
          (S O)
        )

    else (nth_error inode.(block) 14) 
      _flatmap_ (fun tripleIndirectBlock =>
        walkIndirection disk superblock 
          (blockNumInFile - directAddressable
                          - indirect1Addressable
                          - indirect2Addressable)
          (ba2Offset superblock tripleIndirectBlock) 
          (S (S O))
      )
    ) in
    ...
\end{lstlisting}

Finally, once we have the block address for the data block, we need to jump to
the actual byte requested. We determine this simply by taking the byte
position requested modulo the size of each data block.

\begin{lstlisting}
    ...
    blockAddress _flatmap_ (fun blockAddress =>
      (* With the block address, get the individual byte *)
      disk ((ba2Offset superblock blockAddress) 
            + (bytePos mod blockSize))
    ).
\end{lstlisting}

With this function, we have completed each of the components needed to
represent a file structure. Putting them together (via {\tt mkFile}) allows us
to use that file as the evidence needed for much of {\tt
bordland\_honeynet\_file}. In particularly, we know how to prove that the file
{\tt isOnDisk} (if we can provide an Inode Index that matches it), that it the
file {\tt isDeleted} (if the deleted flag is set), and that the file {\tt
isGzip} (if the first three bytes are those of a Gzip.)

All that's left is to prove that there exists some filename within the gzipped
tar that {\tt looksLikeRootkit}.

\begin{lstlisting}
  ...
  /\ exists (filename: ByteString),
     (In filename (Tar.parseFileNames (gunzip_a file)))
     /\ looksLikeRootkit filename.
\end{lstlisting}

\subsection{Parsing File Names from a Tar}
Once we've been given the unzipped tar file (i.e. from {\tt gunzip\_a}) we
need to parse its structure well enough to pull out the file names it
contains. This is not terribly dissimilar to pulling out SuperBlocks, Inodes,
etc. from a Disk, save that here we are working within the context of a single
file. As files are abstracted to the point that they need only provide a
function to access their data, we need not worry about underlying file system
mechanics.

A tar file is composed of a sequence of (header, file content) pairs such that
the header meta data about the file (including its name, size, owner, etc.)
and the file data is padded to a multiple of 512 bytes. To retrieve all of the
file names contained within a tar, we will need to read the ``next'' file name
from the tar header, read the file size from that header, skip the file
contents and start again with the following header.

\subsubsection{File Size from ASCII Octal}
File size can be determined by reading the 11 bytes starting at offset 124 of
the tar header. The size is first encoded into octal, which is then
represented as ASCII characters. Scratching your head yet? We implement this
with a simple recursive function which reads the next byte from a list,
converts that into the encoded integer value. The value is multiplied by 8
raised to the length of rest of the list to account for octal order of
magnitude. If the byte does not fall into the range associated with ascii `0'
through `7', an {\tt error} is returned.

\begin{lstlisting}
Fixpoint fromOctalAscii (bytes: list Z) : Exc Z :=
  match bytes with
  | nil => value 0
  | byte :: tail => match (fromOctalAscii tail) with
    | error => error
    | value rest => if (andb (48 <=? byte) (byte <=? 56))
      then value (rest + ((byte-48)
                          * (8 ^ (Z.of_nat (length tail)))))
      else error (* Invalid character *)
    end
  end.
\end{lstlisting}

\subsubsection{More Functional Idioms}

The first hundred bytes of the header are dedicated to the filename
(regardless of filename length); the filename is null-terminated. To get
there, we will add three new functional idioms: {\tt upto}, {\tt flatten}, and
{\tt takeWhile}. {\tt upto} (or {\tt range}) acts as a simple, infix way to
create a sequence of integers between the provided end points; we skip its
definition as it involves too much Coq-specific inside baseball. Similar to
our need for {\tt \_flatmap\_}, {\tt flatten} takes a list of options and
converts them to a list of the options' contents. Finally {\tt takeWhile} is a
function that takes two parameters, a boolean predicate and a list of
elements. This function returns the longest prefix of that list such that
every element satisfies the predicate.

\begin{lstlisting}
Fixpoint takeWhile {A} (predicate: A->bool) (lst: list A)
  : list A :=
  match lst with
  | head :: tail => 
    if (predicate head) 
    then head :: (takeWhile predicate tail)
    else nil
  | nil => nil
end.

Fixpoint flatten {A} (lst: list (Exc A)): list A :=
  match lst with
    | (value head) :: tail => head :: (flatten tail)
    | error :: tail => flatten tail
    | nil => nil
  end.
\end{lstlisting}

\subsubsection{Parsing a Single Header}
Next, we will write a function that can both pull out the first filename from
a tar, but also pull out the first file. Retrieving the file name requires
reading the first hundred bytes of the tar and taking every character until a
null is read. Due to our abstract concept of a file, extracting the first file
from the tar is relatively straight forward. We need only parse the file size
(as described above), carry over the deleted status of the parent tar file,
and drop the tar header from the data to create a File object.

\begin{lstlisting}
Definition parseFirstFileNameAndFile (tar: File)
  : Exc (ByteString*File) :=
  let firstHundredBytes := map tar.(data) (0 upto 100) in
  let fileName := flatten (takeWhile 
    (fun (byte: Exc Z) => match byte with
      | error => false
      | value 0 => false
      | value _ => true
    end) firstHundredBytes) in
  (seq_list tar.(data) 124 11) 
    _flatmap_ (fun fileSizeList =>
  (fromOctalAscii fileSizeList) _map_ (fun fileSize =>
    (fileName, 
     (mkFile fileSize
             (* Keep the deleted status of the tar *)
             tar.(deleted)  
             (* Header size = 512 *)
             (shift tar.(data) 512)))
  )).
\end{lstlisting}

\subsubsection{Recursing Through}
Now that we have a function which fetches the first filename and file from the
tar, we can call it recursively, creating a new version of the tar file (i.e.
minus the first file) with each step. We need to strip off the initial file
header (512 bytes) plus the file size padded to 512 bytes. We won't use a
fixpoint, opting instead for a parameter ({\tt nextCall}) to signify the
recursive call; this is required by {\tt N.peano\_rect}, a library function
which allows for recursive calls over binary numbers. To see the full
definition of {\tt Tar.parseFileNames}, please see the source code.

\begin{lstlisting}
Definition recFileNameFrom (nextCall: File -> list ByteString) 
  (remaining: File) : list ByteString :=
  if (remaining.(fileSize) <=? 0)
    then nil
  else match (parseFirstFileNameAndFile remaining) with
    | error => nil
    | value (fileName, file) =>
        (* Strip the first file out of the tar *)
        (* Round to the nearest 512 *)
        let firstFileSize := (
          if (file.(fileSize) mod 512 =? 0)
          then file.(fileSize) + 512
          else 512 * (2 + (file.(fileSize) / 512))) in
        let trimmedTar := 
          (mkFile (remaining.(fileSize) - firstFileSize)
                  remaining.(deleted) (* Parent's value *)
                  (shift remaining.(data) firstFileSize)) in
        fileName :: (nextCall trimmedTar)
    end.
\end{lstlisting}

\subsection{Computing Over File Names}
Now that we have pulled the list of file names from the tar file, we want the
ability to prove that at least one {\tt looksLikeRootkit}. As we discussed
before, this means that we want to compare the file name's suffix with one of
a set of file names:

\begin{lstlisting}
Definition looksLikeRootkit (fileName: ByteString) :=
  In (trimFileNamePrefix fileName)
     (map ascii2Bytes ("ps" :: "netstat" :: "top" :: "ifconfig" 
                       :: "ssh" :: "rsync" :: "ProcMon.exe"
                       :: nil)).
\end{lstlisting}

The {\tt ascii2Bytes} function simply converts each Coq ASCII character into a
corresponding {\tt Z} to represent that as a byte. We use the functional {\tt
map} idiom to run this function an every element of the list of Coq strings.

\begin{lstlisting}
Fixpoint ascii2Bytes (fileName: string): ByteString :=
  match fileName with
  | EmptyString => nil
  | String char tail => 
    (Z.of_N (N_of_ascii char)) :: (ascii2Bytes tail)
  end.
\end{lstlisting}

We also need to define the {\tt trimFileNamePrefix} function, which we want to
return the string after the last directory delimiter (i.e. after the last `/'
or '$\backslash$'). To do this, we reuse our {\tt takeWhile} function applied
to the reverse of the file name provided, taking bytes until we hit either `/'
or `$\backslash$'. We then reverse the result to get the final, trimmed file
name.

\begin{lstlisting}
Definition trimFileNamePrefix (fileName: ByteString)
  : ByteString :=
  let reversedName := rev fileName in
  rev (takeWhile 
        (fun (char: Z) => 
          (negb (orb (char =? 47)    (* '/' *)
                     (char =? 92)))) (* '\' *)
        reversedName).
\end{lstlisting}

With that, we have described all of the components needed to compute our
lemma. We leave the details of the proof to our code samples, but at the high
level, one can imagine how proving {\tt borland\_honeynet\_file} need only
require we provide a File (by parsing it from the disk) as evidence. We can
more or less compute each of the properties from that file.

\section{Timelines as Evidence}

A second type of evidence provided by the Honeynet researchers (particularly
from Jason Lee\cite{lee}) was in the form of a timeline of events to explain
what the researchers believed happened to the infected server. A timeline is
simply an ordered sequence of events; examples of events include file
modification, user login, system restart, etc. Timelines are certainly useful
as a form of evidence as they provide a narrative of what took place, but they
are also provable artifacts, as a timeline is only sound as long as there is
evidence for each of the events and if the order of those events can be
verified. In this case, we say that the events are in the correct sequence if
events earlier in the sequence are {\tt beforeOrConcurrent} with events later
in the sequence.

\begin{lstlisting}
Definition isSound (timeline: Timeline) (disk: Disk) :=
  (forall (event: Event),
    (* Event is evident from the disk *)
    (In event timeline) -> (foundOn event disk))
  (* Events are in the correct sequence *)
  /\ (forall (index: nat),
        (index < ((length timeline) - 1) )%nat ->
          match (nth_error timeline index,
                 nth_error timeline (index + 1)) with
          | (value lhsEvent, value rhsEvent) => 
            beforeOrConcurrent lhsEvent rhsEvent
          | _ => False
          end)
  .
\end{lstlisting}

\subsection{Events and Their Relations}

We next consider an {\tt Event} type and its various forms. For files, we will
see four events: access, creation, modification, and deletion. We will
represent these events with two parameters, a unix-style timestamp of the
event's execution and a unique identifier for the file (e.g. Inode Index in
Ext2).

\begin{lstlisting}
Inductive Event: Type :=
  | FileAccess: Z -> Z -> Event
  | FileModification: Z -> Z -> Event
  | FileCreation: Z -> Z -> Event
  | FileDeletion: Z -> Z -> Event
\end{lstlisting}

While not all conceivable events have a timestamp, those that do make the {\tt
beforeOrConcurrent} definition significantly simpler. To prove that one event
happens {\tt beforeOrConcurrent} another, simply compare the timestamps.

\begin{lstlisting}
Definition timestampOf (event: Event) : Exc Z :=
  match event with
  | FileAccess timestamp _ => value timestamp
  | FileModification timestamp _ => value timestamp
  | FileCreation timestamp _ => value timestamp
  | FileDeletion timestamp _ => value timestamp
  | _ => error.

Definition beforeOrConcurrent (lhs rhs: Event) :=
  match (timestampOf lhs, timestampOf rhs) with
  | (value lhs_time, value rhs_time) => lhs_time <= rhs_time
  | _ => False.
\end{lstlisting}

Note that the {\tt beforeOrConcurrent} relation need not be limited to Events
where we have a concrete timestamp. We could extend the definition of an Event
to include execution of shell scripts, for example, which have a clear
relative ordering of timestamps but do not have absolute time.

We also define a function which pulls out the inode of an event (as this is a
common operation amongst the four types of Events we have defined.) This is
almost identical to the timestamp retrieval. 

\begin{lstlisting}
Definition inodeIndexOf (event: Event) : Exc Z :=
  match event with
  | FileAccess _ inodeIndex => value inodeIndex
  | FileModification _ inodeIndex => value inodeIndex
  | FileCreation _ inodeIndex => value inodeIndex
  | FileDeletion _ inodeIndex => value inodeIndex
  end.
\end{lstlisting}

\subsection{Finding Events and Their Existence}

For a Timeline to be valid, each of the events in the timeline must follow
from the disk image. As the events we have discussed so far have an associated
Inode, it's easy to compute the Inode structure and verify that its access,
modification, change, or deletion time match that of the event.

\begin{lstlisting}
Definition foundOn (event: Event) (disk: Disk) : Prop :=
  match (inodeIndexOf event) _flatmap_ (fun inodeIndex =>
    (ext2_inode_from disk inodeIndex) _map_ (fun inode =>
      match event with
      | FileAccess timestamp _ => 
        inode.(accessTime) = timestamp
      | FileModification timestamp _ => 
        inode.(modificationTime) = timestamp
      | FileCreation timestamp _ => 
        inode.(changeTime) = timestamp
      | FileDeletion timestamp _ => 
        inode.(deletionTime) = timestamp
      end
    )
  ) with
  | error => False
  | value prop => prop
  end.
\end{lstlisting}

\subsection{Applying to Honeynet}
With all of these definitions we can now see how one would provide evidence
that a particular timeline was valid. Consider Jason Lee's entry\cite{lee}
into the Honeynet contest described before. As part of his evidence, he
provided a sequenced list of inode events (as discovered by ``MACtimes'') and
annotated their significance. We copy several of these events and their
annotations into Coq; in the conversion we lose file name (instead, we use
inode number) and pretty-printed dates (opting for unix time stamps).

\begin{lstlisting}
Lemma lee_honeynet_file:
  (Timeline.isSound (
    (* Mar 16 01 12:36:48 *)
      (* rootkit lk.tar.gz downloaded *)
        (FileModification 984706608 23)
    (* Mar 16 01 12:44:50 *)
      (* Gunzip and Untar rootkit lk.tar.gz *)
        :: (FileAccess 984707090 23)
      (* change ownership of rootkit files to root.root *)
        :: (FileAccess 984707102 30130)
      (* deletion of original /bin/netstat *)
        :: (FileDeletion 984707102 30188)
      (* insertion of trojan netstat *)
        :: (FileCreation 984707102 2056) 
      (* deletion of original /bin/ps *)
        :: (FileDeletion 984707102 30191)
      (* insertion of trojan ps *)
        :: (FileCreation 984707102 2055) 
      (* deletion of origin /sbin/ifconfig *)
        :: (FileDeletion 984707102 48284)
      (* insertion of trojan ifconfig *)
        :: (FileCreation 984707102 2057) 
    (* Mar 16 01 12:45:03 *)
      (* the copy of service files to /etc *)
        :: (FileAccess 984707103 30131)  
      (* hackers services file copied on top of original *)
        :: (FileCreation 984707103 26121)
    (* Mar 16 01 12:45:05 *)
      (* deletion of rootkit lk.tar.gz *)
        :: (FileDeletion 984707105 23)   
    :: nil)
    honeynet_image_a
  ).
\end{lstlisting}

Seeing as the definition of {\tt Timeline.isSound} is composed of two
computable relations, proving this lemma boils down to a few computations. As
before, please see the source code for full details.

\section{Discussion}
In this paper, we have provided several definitions for types of evidence that
would be applicable to forensics researchers. It is important to take a step
back and determine whether or not those definitions provide the evidence we
need to prove that a root kit was installed on the disk in question. 

We might ask if there are other scenarios that would lead to our definition
being satisfied without a rootkit being installed; in other words, what kind
of false positives could occur? Thinking back to our definition, we might
question each of the clauses within the {\tt borland\_honeynet\_file}
conjunction. Assuming we weren't dealing with an Ext2 file system, it is
possible that a very unfortunate, but random disk image could match the bytes
of the Ext2 disk, therefore satisfying {\tt isOnDisk} even though there is no
file system to speak of. Similarly, a random sequence of bytes could also
satisfy our {\tt isDeleted} requirement and even our {\tt isGzip} check.

Examining the {\tt looksLikeRootkit} function provides another avenue for
questioning the legitimacy of our work. Many, perfectly kosher file names
might trigger the root kit alarm if this were the only definition we used, as
harmless files could be named ssh, ps, ifconfig, etc.

Similarly, we could argue that the timeline we provided could satisfy a
completely different scenario. The annotations lead us down the path to
believing that a root kit had been installed, but we could provide
alternatives which might imply that {\em requested} software had been
installed.

Ultimately, each piece of evidence is not conclusive; we instead rely on the
evidence in aggregate to determine what happened. This, too, is the
methodology found within the Honeynet competition; as there were no canonical
definitions for what it means for a root kit to be installed, each participant
provided his or her own evidence which satisfied his or her own definition.
This led them to describe slightly different attacks, even though they all
shared the same disk image.

We did not set out to provide bullet-proof definitions for concepts such as
deletion, file creation, etc. Instead, we wanted to model the same types of
evidence currently put forth by forensics researchers. If a consensus can be
reached regarding these definitions, we would use it, but at the moment we
model only the de facto standards.

\section{Future Work}
Anyone not already familiar with Coq's tactic systems would be quite confused
by the steps needed to prove any of these lemmas. On the other hand, those who
are familiar with Coq might find the compute-heavy nature of these definitions
to be outside of their comfort zone. Compound that with the rather large
numbers needed, and it becomes safe to say that very few people would be able
to use this system, and none work in the professional forensics space.

A potential solution appears in two phases. First, these proofs are ripe for
automation. Providing one of a pre-defined set of evidence types and a disk
image should be enough for a script to generate the required assumptions,
evidence, and proof terms. As a second step, we could develop a meta language
or other interface for describing the types of evidence needed and allow a
program to search and generate proofs as needed. This connects with wider work
by Radha Jagadeesan, Corin Pitcher, and James Riely of DePaul University,
which includes efforts to automate forensic methods.

This paper describes only a few of the types of evidence that would be
required for a complete forensics tool. We used a single honeynet challenge to
provide this paper scope. Obviously, creating additional definitions of
evidence would be a key area for future research. This means both tasks such
as defining additional file systems and describing additional types of events.
Imagine what types of events would describe a user logging in, executing a
command, and then disconnecting. Imagine the evidence needed to prove that a
user frequently visited a certain domain.

Finally, the vast majority of this paper has been devotes to computations or
definitions; we have not described many relations between the definitions. It
would be worthwhile to proof propositions about these definition (as we proved
that JPEG files cannot be Tars). We might prove that a patch applied to a disk
image could restore a deleted file (as studied by Charles Winebrinner), that
deletion events imply the associated file is deleted, that files can exist
which are not addressable, or any number of other proofs and lemmas. These
would then make proofs about particular disk images even easier to apply.

\acks
First, I wish to thank my colleagues at DePaul, including Malik Aldubayan,
Iana Boneva, Christina Ionides, Daria Manukian, Matthew McDonald, and Charles
Winebrinner for their ideas and community. I would also like to thank my
professors, Radha Jagadeesan, Corin Pitcher, and James Riely for their
feedback and insights. Most notably, working with my advisor, Corin, has been
a great pleasure. Most of the code described comes from kernels he provided,
and I certainly would still be debugging Gecode if he hadn't stepped me
through dozens of proofs. Thank you, all.

Thanks and great appreciation also go to my partner, Laura Cathey, who has
tolerated my absence far too long. I promise I won't start another project for
at least a few days.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem{borland-honeynet}
  Borland, Matt. Submission to Honeynet.org \emph{Scan of the Month},
  05/05/2001. \url{http://old.honeynet.org/scans/scan15/som/som6.txt}
\bibitem{honeynet}
  The Honeynet Project. \url{http://www.honeynet.org/}
\bibitem{honeynet-15}
  The Honeynet Project \emph{Scan of the Month} \#15.
  \url{http://old.honeynet.org/scans/scan15/}
\bibitem{lee}
  Lee, Json. Submission to Honeynet.org \emph{Scan of the Month}, 05/25/2001.
  \url{http://old.honeynet.org/scans/scan15/som/som33.html}
\bibitem{non-gnu}
  Poirier, Dave. The Second Extended File System.
  \url{http://www.nongnu.org/ext2-doc/ext2.html}
\bibitem{sleuth-kit}
  The Sleuth Kit. \url{http://www.sleuthkit.org/}
\bibitem{scala}
  Wampler, Dean and Payne, Alex (2008) Chapter 8. Functional Programming in
  Scala. {\em Programming Scala}. 
  \url{http://ofps.oreilly.com/titles/9780596155957/FunctionalProgramming.html}

\end{thebibliography}

\appendix

\section{JPEG Files Are Not Gzips}
\begin{lstlisting}
Lemma jpeg_is_not_gzip : forall (file: File),
  (isJpeg file) -> ~ (isGzip file).
Proof.
  unfold isGzip, isJpeg.
  intros file jpeg_asmpt.
  destruct jpeg_asmpt as [byte0_is_255]. rewrite byte0_is_255.
  unfold not. intros contra. destruct contra as [not_equal].
  discriminate not_equal.
Qed.
\end{lstlisting}

\section{Looks Like Rootkit Examples}
\begin{lstlisting}
Lemma rootkit_ex1 : 
  looksLikeRootkit (ascii2Bytes "last/top").
Proof.
  compute. right. right. left. reflexivity.
Qed.

Lemma rootkit_ex2 : 
  ~(looksLikeRootkit (ascii2Bytes "last/example.txt")).
Proof.
  compute. intro.
  repeat (destruct H; [ inversion H | ]).
  apply H.
Qed.
\end{lstlisting}

\end{document}
